#!/bin/sh -e

##########################################################################
#   Script description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2014-11-05  root        Begin
##########################################################################

usage()
{
    printf "Usage: $0 output-directory\n"
    exit 1
}


##########################################################################
#   Function description:
#       Pause until user presses return
##########################################################################

pause()
{
    local junk
    
    printf "Press return to continue..."
    read junk
}


##########################################################################
#   Main
##########################################################################

case $# in
1)
    ;;
*)
    usage
    ;;
esac

dir="$1"

readonly lock_file="$dir/lockfile"
if [ -e "$lock_file" ]; then
    printf "Another instance of $0 is currently running.\n"
    printf "Remove $lock_file to override.\n"
    exit
fi
touch "$lock_file"

readonly updated_nodes_file="$dir/updated-nodes"
if [ ! -e "$updated_nodes_file" ]; then
    touch "$updated_nodes_file"
fi

if test -t 0; then
    cat << EOM

All nodes will be set to a draining state and will not accept new jobs
until they have been updated.

If there are jobs currently running, this script will need to be run
repeatedly until all current jobs are finished in order to update all nodes.

To ensure that all future jobs run on nodes at the same patch level, you
must allow this round of updates to complete before starting a new round.

To start a new round of updates, simply delete the file

    $updated_nodes_file

EOM
    pause
fi

cd "$dir"
sinfo -o '%n %T' | sort | uniq
wc "$updated_nodes_file"
updated=0

if [ `squeue | awk '$5 == "S"' | wc -l` != 0 ]; then
    printf "There are suspended jobs.  It is not safe to run updates.\n"
    exit 1
fi

nodes=`sinfo -o %n -h | sort | uniq`

# First pass simply drains as many nodes as possible, as quickly as possible,
# to prevent jobs from allocating them before they're updated.
for node in $nodes; do
    # Update node if it's not in the updated list
    if ! fgrep -q $node "$updated_nodes_file"; then
	state=`sinfo -n $node -o %T -h`
	case $state in
	'drained'|'drained*')
	    ;;
	
	'down'|'down*')
	    # Don't change state of down nodes!
	    ;;
	
	*)
	    scontrol update nodename=$node state=drain \
		reason=slurm-update-idle-nodes
	    ;;
	esac
    fi
done

# Second pass will update already drained nodes.
for node in $nodes; do
    # Update drained nodes not in the updated list
    if ! fgrep -q $node "$updated_nodes_file"; then
	state=`sinfo -n $node -o %T -h`
	case $state in
	'drained'|'drained*')
	    printf "Updating $node...\n"
	    # Why was this here?
	    #scontrol update nodename=$node state=down \
	    #    reason=slurm-update-idle-nodes
	    
	    # Install all available binary updates and reboot
	    if ssh -t $node auto-update-system --binary+reboot; then
		echo $node >> "$updated_nodes_file"
		
		# shutdown may take some time to disable sshd
		# This will prevent slurm-resume-down-nodes from
		# resuming the node before the reboot.  It may also
		# prevent reason from being set to "Node unexpectedly rebooted".
		while ping -c 1 -q $node > /dev/null 2>&1; do
		    printf "Verifying reboot of $node...\n"
		    sleep 5
		done
		
		# This should cause the node to get set to down state
		# without changing "reason", assuming the node is unreachable
		# when this is run.
		scontrol update nodename=$node state=resume
	    else
		printf "Update failed.\n"
	    fi
	    ;;

	'down')
	    # Don't change state of down nodes!
	    ;;
	    
	*)
	    # If not updated and not drained, drain it now
	    scontrol update nodename=$node state=drain \
		reason=slurm-update-idle-nodes
	    ;;
	esac
    fi
    
    # Resume all nodes that have been updated but are still marked down
    slurm-resume-updated-nodes
done
wc updated-nodes
rm -f "$lock_file"

