#!/bin/sh -e

##########################################################################
#   Script description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2014-11-05  root        Begin
##########################################################################

usage()
{
    printf "Usage: $0 output-directory\n"
    exit 1
}


##########################################################################
#   Function description:
#       Pause until user presses return
##########################################################################

pause()
{
    local junk
    
    printf "Press return to continue..."
    read junk
}


##########################################################################
#   Main
##########################################################################

case $# in
1)
    ;;
*)
    usage
    ;;
esac

dir="$1"

readonly lock_file="$dir/lockfile"
if [ -e "$lock_file" ]; then
    printf "Another instance of $0 is currently running.\n"
    printf "Remove $lock_file to override.\n"
    exit
fi
touch "$lock_file"

readonly updated_nodes_file="$dir/updated-nodes"
if [ ! -e "$updated_nodes_file" ]; then
    touch "$updated_nodes_file"
fi

if test -t 0; then
    cat << EOM

All nodes will be set to a draining state and will not accept new jobs
until they have been updated.

If there are jobs currently running, this script will need to be run
repeatedly until all current jobs are finished in order to update all nodes.

To ensure that all future jobs run on nodes at the same patch level, you
must allow this round of updates to complete before starting a new round.

To start a new round of updates, simply delete the file

    $updated_nodes_file

EOM
    pause
fi

cd "$dir"
sinfo -o '%n %T'
wc "$updated_nodes_file"
updated=0

if [ `squeue | awk '$5 == "S"' | wc -l` != 0 ]; then
    printf "There are suspended jobs.  It is not safe to run updates.\n"
    exit 1
fi

nodes=`sinfo -o %n -h`
# First pass simply drains as many nodes as possible to prevent more jobs
# from allocating them.
for node in $nodes; do
    # Update node if it's not in the updated list
    if ! fgrep -q $node "$updated_nodes_file"; then
	state=`sinfo -n $node -o %T -h`
	case $state in
	'drained')
	    ;;
	*)
	    scontrol update nodename=$node state=drain reason=Upgrading
	    ;;
	esac
    fi
done

# Second pass will update already drained nodes.
for node in $nodes; do
    # Update node if it's not in the updated list
    if ! fgrep -q $node "$updated_nodes_file"; then
	state=`sinfo -n $node -o %T -h`
	case $state in
	'drained')
	    printf "Updating $node.   State = $state...\n"
	    if ssh -t $node auto-update-system; then
		scontrol update nodename=$node state=resume
		echo $node >> "$updated_nodes_file"
	    else
		printf "Update failed.\n"
	    fi
	    ;;
	*)
	    scontrol update nodename=$node state=drain reason=Upgrading
	    ;;
	esac
    fi
done
wc updated-nodes

# FIXME: Can SLURM be configured to resume nodes automatically
# after they reboot?
slurm-resume-down-nodes > /dev/null 2>&1
rm -f "$lock_file"

