#!/bin/sh -e

##########################################################################
#   Script description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2015-06-16  root        Begin
##########################################################################

usage()
{
    printf "Usage: $0 [--use-defaults] [--infiniband] host.domain head|compute|io\n"
    exit 1
}


##########################################################################
#   Function description:
#       Pause until user presses return
##########################################################################

pause()
{
    local junk
    
    printf "Press return to continue..."
    read junk
}


##########################################################################
#   Main
##########################################################################

if [ 0$1 = 0--use-defaults ]; then
    use_defaults=yes
    shift
fi
if [ 0$1 = 0--infiniband ]; then
    setup_flags=--infiniband
    shift
fi

if [ $# != 2 ]; then
    usage
fi

LOCALBASE=/usr/local

long_node_name=$1
if ! echo $long_node_name | fgrep -q '.'; then
    printf "No domain name in $long_node_name.  Continue? y/[n] "
    read continue
    if [ 0$continue != 0y ]; then
	exit 1
    fi
fi

node_type=$2
case $node_type in
head|compute|io)
    ;;
*)
    usage
esac

node=${long_node_name%%.*}

if [ -z EDITOR ]; then
    EDITOR="vi"
fi

conf_dir='/usr/local/cluster'
if ! ssh $node stat $conf_dir/init-done; then
    printf "You must run 'cluster-init-node $long_node_name $node_type' first.\n"
    exit 1
fi

case `auto-ostype` in
RHEL)
    # FIXME: Prevent node from being enabled in the scheduler
    # until setup is complete

    ssh $node pkgin install -y rsync
    ssh $node yum remove -y rsync

    # Install basic tools and updates
    
    if [ 0$use_defaults = 0yes ]; then
	sync_yum=n
    else
	cat << EOM
---
Answer 'y' here if you have distributed new Yum packages while this node
was inaccessible.
---
EOM
	read -p "Sync Yum packages? y/[n] " sync_yum
    fi
    if [ 0$sync_yum = 0y ]; then
	read -p "Reference node for Yum packages? " ref_node
	cluster-sync-yum-packages $ref_node $node
    fi
    
    # Sync users
    if [ 0$use_defaults = 0yes ]; then
	sync_users=y
    else
	cat << EOM
---
Answer 'y' here if you have added new users while this node was inaccessible.
---
EOM
	read -p "Sync users? [y]/n " sync_users
    fi
    if [ 0$sync_users != 0n ]; then
	node-sync-all-users $node || true
    fi
    
    # Not in the UID range for normal cluster users, so node-sync-all-users
    # will skip it.
    # FIXME: Don't assume slurm
    node-sync-users $node slurm || true
    
    # Sync sys files
    cluster-sync-sysfiles $node
    
    cluster-sync-files $node
    
    # hosts.allow
    rsync -av /etc/hosts.allow ${node}:/etc
    
    # Munge key
    rsync -av /usr/pkg/etc/munge ${node}:/usr/pkg/etc
    
    # Ganglia
    rsync -av /usr/pkg/etc/gmond.conf ${node}:/usr/pkg/etc
    
    # SLURM config
    for file in slurm.conf cgroup.conf; do
	if [ -e /usr/pkg/etc/$file ]; then
	    rsync -av /usr/pkg/etc/$file ${node}:/usr/pkg/etc
	fi
    done
    
    # Install standard software packages
    yum_list=/usr/local/cluster/$node_type-node-yum-packages
    if [ -e $yum_list ]; then
	printf "Installing local packages from $yum_list...\n"
	ssh $node yum install -y `cat $yum_list`
    else
	printf "No $yum_list found.\n"
    fi

    # Run cluster-setup $node_type
    # Do this last, since it enables the scheduler
    if [ 0$use_defaults = 0yes ]; then
	ssh -t $node env AUTO_ASK_USE_DEFAULTS=yes cluster-setup $setup_flags $node_type
    else
	ssh -t $node cluster-setup $setup_flags $node_type
    fi

    printf "Be sure to wait until the node reboots before restarting SLURM.\n"
    ;;

FreeBSD)
    # Install basic tools and updates

    # Should be preinstalled by cluster-init-node
    ssh -t $node wip-update
    
    # Sync users
    node-sync-all-users $node || true
    
    # Not in the UID range for normal cluster users, so node-sync-all-users
    # will skip it.
    # FIXME: Don't assume slurm
    node-sync-users $node slurm || true
    
    # Sync sys files
    cluster-sync-sysfiles $node
    
    cluster-sync-files $node
    
    # Sync core packages
    if [ 0$use_defaults = 0yes ]; then
	sync_packages=n
    else
	read -p "Sync packages? y/[n] " sync_packages
    fi
    if [ 0$sync_packages = 0y ]; then
	read -p "Source node? " source_node
	cluster-sync-packages $source_node $node
    fi
    
    # hosts.allow
    rsync -av /etc/hosts.allow ${node}:/etc
    
    # Ganglia
    rsync -av /usr/local/etc/gmond.conf ${node}:/usr/local/etc
    
    # Could use munge_flags="--key-file $HEAD_USR/local/etc/munge/munge.key"
    munge_dir="$LOCALBASE/etc/munge"
    ssh $node mkdir -p -m 0700 $munge_dir
    scp -p $munge_dir/munge.key ${node}:$munge_dir
    
    slurm_conf="$LOCALBASE/etc/slurm.conf"
    scp $slurm_conf ${node}:$slurm_conf
    
    # Run cluster-setup $node_type
    # Do this last, since it enables the scheduler
    printf "\nRunning cluster-setup $node_type on $node...\n\n"
    if [ 0$use_defaults = 0yes ]; then
	ssh -t $node env AUTO_ASK_USE_DEFAULTS=yes cluster-setup $setup_flags $node_type
    else
	ssh -t $node cluster-setup $setup_flags $node_type
    fi
    
    printf "Run cluster-sync-sysfiles to update other nodes if you've updated /etc/hosts.\n"
    printf "Be sure to wait until the node reboots before restarting SLURM.\n"
    pause
    ;;

*)
    printf "Not yet implemented for `auto-ostype`.\n"
    exit 1
    ;;

esac
