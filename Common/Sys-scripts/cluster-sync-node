#!/bin/sh -e

##########################################################################
#   Script description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2015-06-16  root        Begin
##########################################################################

usage()
{
    printf "Usage: $0 host.domain compute|io\n"
    exit 1
}


##########################################################################
#   Function description:
#       Pause until user presses return
##########################################################################

pause()
{
    local junk
    
    printf "Press return to continue..."
    read junk
}


##########################################################################
#   Main
##########################################################################

if [ $# != 2 ] || [ 0$2 != 0'compute' ] && [ 0$2 != 0'io' ]; then
    usage
fi

LOCALBASE=/usr/local

long_node_name=$1
if ! echo $long_node_name | fgrep -q '.'; then
    printf "No domain name in $long_node_name.  Continue? y/[n] "
    read continue
    if [ 0$continue != 0y ]; then
	exit 1
    fi
fi
node_type=$2

node=${long_node_name%%.*}

case $node_type in
backup|compute|io)
    ;;
*)
    usage
esac

if [ -z EDITOR ]; then
    EDITOR="vi"
fi

conf_dir='/usr/local/cluster'

case `auto-ostype` in
RHEL)
    # FIXME: Prevent node from being enabled in the scheduler

    # Install basic tools and updates
    # cluster-init-node $long_node_name $node_type
    if ! ssh $node stat $conf_dir/init-done; then
	printf "You must run cluster-init-node $long_node_name first.\n"
	exit 1
    fi
    
    printf "Reference node for Yum packages? "
    read ref_node
    cluster-sync-yum-packages $ref_node $node
    
    # Sync users
    printf "Sync users? [y]/n "
    read sync_users
    if [ 0$sync_users != 0n ]; then
	node-sync-all-users $node || true
    fi
    
    # Not in the UID range for normal cluster users, so node-sync-all-users
    # will skip it.
    # FIXME: Don't assume slurm
    node-sync-users $node slurm || true
    
    # Sync sys files
    cluster-sync-sysfiles $node
    
    cluster-sync-files $node
    
    # hosts.allow
    rsync -av /etc/hosts.allow ${node}:/etc
    
    # Other stuff
    
    # Sync software
    yum_list=/usr/local/cluster/$node_type-node-yum-packages
    if [ -e $yum_list ]; then
	printf "Installing local packages from $yum_list...\n"
	ssh $node yum install -y `cat $yum_list`
    else
	printf "No $yum_list found.\n"
    fi

    # Run cluster-setup compute
    # Do this last, since it enables the scheduler
    ssh -t $node cluster-setup $node_type
    
    printf "Be sure to wait until the node reboots before restarting SLURM.\n"
    ;;

FreeBSD)
    # Install basic tools and updates
    # cluster-init-node $long_node_name $node_type
    if ! ssh $node stat $conf_dir/init-done; then
	printf "You must run cluster-init-node $long_node_name first.\n"
	exit 1
    fi
    
    # FIXME: Replace with pkg install cluster-admin when possible
    # working around hwloc seg fault bug
    ssh $node 'fgrep -q USE_LOCAL_MK /etc/make.conf || printf "USE_LOCAL_MK=yes\n" >> /etc/make.conf'
    ssh $node 'cd /usr/ports/wip/wip-tools && make clean deinstall reinstall'
    ssh -t $node wip-update
    ssh $node 'cd /usr/ports/wip/cluster-admin && make -DBATCH deinstall reinstall'
    ssh $node pkg install -y hwloc
    #ssh $node 'cd /usr/ports/wip/hwloc && make -DBATCH deinstall reinstall'
    
    # Add new node to ssh_config on all hosts
    cluster-update-ssh_config
    ssh $node cluster-update-ssh_config
    
    # Sync users
    node-sync-all-users $node || true
    
    # Not in the UID range for normal cluster users, so node-sync-all-users
    # will skip it.
    # FIXME: Don't assume slurm
    node-sync-users $node slurm || true
    
    # Sync sys files
    cluster-sync-sysfiles $node
    
    cluster-sync-files $node
    
    printf "Sync packages? [y]/n "
    read sync
    if [ 0$sync != 0n ]; then
	read -p "Source node? " source_node
	cluster-sync-packages $source_node $node
    fi
    
    # hosts.allow
    rsync -av /etc/hosts.allow ${node}:/etc
    
    # Other stuff
    
    # Sync software
    # FIXME: Implement for FreeBSD
    
    # Could use munge_flags="--key-file $HEAD_USR/local/etc/munge/munge.key"
    munge_dir="$LOCALBASE/etc/munge"
    ssh $node mkdir -p -m 0700 $munge_dir
    scp -p $munge_dir/munge.key ${node}:$munge_dir
    
    slurm_conf="$LOCALBASE/etc/slurm.conf"
    scp $slurm_conf ${node}:$slurm_conf
    
    # Run cluster-setup compute
    # Do this last, since it enables the scheduler
    printf "\nRunning cluster-setup $node_type on $node...\n\n"
    ssh -t $node cluster-setup $node_type
    
    printf "Run cluster-sync-sysfiles to update other nodes if you've updated /etc/hosts.\n"
    printf "Be sure to wait until the node reboots before restarting SLURM.\n"
    pause
    ;;

*)
    printf "Not yet implemented for `auto-ostype`.\n"
    exit 1
    ;;

esac
