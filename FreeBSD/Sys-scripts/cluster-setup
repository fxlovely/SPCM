#!/bin/sh -e

##########################################################################
#   Description:
#       Automate setup of a simple FreeBSD cluster.
#       Installs common software and configures settings to facilitate
#       cluster operation.  Assumes a single head node will handle
#       job initiation/scheduling and act as a file server for all nodes.
#
#   Usage:
#       First, run 
#
#           cluster-setup head
#
#       on the head node.
#
#   Then, run
#
#           cluster-setup compute
#
#       on the rest.
#       
#   History:
#       Dec 2009    J Bacon
##########################################################################


usage()
{
    printf "Usage: $0 head|compute\n"
    exit 1
}


line()
{
    printf "==============================================================================\n"
}


pause()
{
    local junk
    printf "Press enter to continue..."
    read junk
}


# FIXME: Base this on stop_service
stop_pbs()
{
    printf "Stopping PBS daemons...\n"
    if fgrep -q pbs_sched_enable $RC_CONF; then
	service pbs_sched stop > /dev/null 2>&1 || true
	sleep 2
    fi
    if fgrep -q pbs_server_enable $RC_CONF; then
	service pbs_server stop > /dev/null 2>&1 || true
	sleep 2
    fi
    if killall pbs_sched > /dev/null 2>&1; then
	printf "Rogue pbs_sched terminated.\n"
	sleep 2
    fi
    if killall pbs_server > /dev/null 2>&1; then
	printf "Rogue pbs_server terminated.\n"
	sleep 2
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-03  Charlie &   Begin
##########################################################################

stop_service()
{
    if [ $# != 1 ]; then
	printf "Usage: stop_service service\n"
    fi
    service $1 stop > /dev/null 2>&1 || true
    killall $1 > /dev/null 2>&1 || true
}


##########################################################################
#   Description:
#       Install ports or packages specified by arguments if they are not
#       already installed.  If $AUTO_BUILD_FROM_SOURCE is true, installs from source,
#       otherwise attempts to install using pkg add.
#
#   Arguments:
#       List of ports in the for "category/name"
##########################################################################

install_packages()
{
    printf "Installing:\n"
    for pkg in $*; do
    {
	printf "\t$pkg...\n"
	if [ ! -e $LOG_DIR ]; then
	    mkdir $LOG_DIR
	fi
	log_file=$LOG_DIR/`echo $pkg | tr / -`
	if ! auto-install-packages $pkg > $log_file 2>&1; then
	    more $log_file
	    exit 1
	fi
    }
    done
}


install_port()
{
    local resp category port
    
    if [ $# -lt 2 ]; then
	printf "Usage: install_port category port [make flags]\n"
	exit 1
    fi
    
    category=$1
    port=$2
    shift
    shift
    
    # FIXME: Use cluster-pkgdir
    cd /usr/ports/$category/$port
    make deinstall
    make clean
    make rmconfig
    make reinstall
}


add_node_name()
{
    if [ $# != 2 ]; then
	printf "Usage: $0 node-name node-file\n"
	exit 1
    fi
    
    name=$1
    file=$2
    
    # Prevent fgrep from failing
    if [ ! -e $file ]; then
	touch $file
    fi
    
    if ! fgrep -q $name $file; then
	printf "$name\n" >> $file
    fi
}


install_scheduler()
{
    line
    # Patch bug in openmpi run_depends
    sed -i '.bak' 's|${SGE_ROOT}/fbsd-${ARCH}/qrsh|${SGE_ROOT}/bin/fbsd-${ARCH}/qrsh|g' \
	/usr/ports/net/openmpi/Makefile
    
    cat << EOM
Please choose a scheduler:

1.. HTCondor
2.. SGE (Requires X11)
3.. SLURM
4.. Torque PBS

EOM

    resp=' '
    while [ 0$resp != 01 ] && [ 0$resp != 02 ] && [ 0$resp != 03 ] && [ 0$resp != 04 ]; do
	resp=`auto-ask scheduler "Scheduler selection?" unknown`
	case $resp in
	1)
	    install_packages sysutils/condor
	    pause
	    ;;
	2)
	    if ! auto-package-installed sysutils/sge62; then
		install_packages sysutils/sge62
	    
		line
		printf "You can ignore the post-install messages above.\n"
		printf "The services will be configured automatically.\n"
		pause
		
		# Source $LOCALBASE/sge/default/common/settings.csh
		# All execution hosts must be administrative hosts during the installation.
		# All hosts which you added to the list of administrative hosts during this
		# installation procedure can now be installed.
		line
		printf "\nAll users should source $LOCALBASE/sge/<cell>/common/settings.[c]sh\n\n"
		pause
	    else
		printf "SGE is already installed.\n"
	    fi
	    ;;
	3)
	    # Install dependencies
	    pkg install -y slurm-hpc
	    
	    # Reinstall with custom options
	    reinstall=`auto-ask reinstall-slurm "Rebuild slurm with custom options?" n`
	    if [ $reinstall = 'y' ]; then
		install_port sysutils slurm-hpc
	    fi
	    ;;
	4)
	    if ! auto-package-installed sysutils/torque; then
		line
		install_packages sysutils/torque
		# Package does not set permissions on spool directories
		# Also, users are encouraged to update sysutils/torque
		# to the latest version instead of using the package
		#pkg_delete /var/db/pkg/torque*
		#install_packages lang/perl devel/libtool
		#printf "Reinstalling torque from port to ...\n"
		#install_port sysutils torque
	    
		line
		printf "You can ignore the post-install messages above.\n"
		printf "The services will be configured automatically.\n"
		pause
	    fi
	    ;;
	*)
	    printf "\nInvalid selection.\n\n"
	    pause
	    ;;
	esac
    done
}


install_mpi()
{
    line
    # Install OpenMPI from source so we can choose build options
    if ! auto-package-installed net/openmpi; then
	#if [ $AUTO_BUILD_FROM_SOURCE = 0 ]; then
	    printf "\nInstalling openmpi dependencies from packages...\n"
	    install_packages net/openmpi
	    # FIXME: use auto-delete-packages
	    # pkg_delete /var/db/pkg/openmpi*
	#fi
	
	reinstall_mpi=`auto-ask rebuild-mpi "Reinstall openmpi from port to customize build?" y`
	if [ $reinstall_mpi = 'y' ]; then
	    pkg delete openmpi
	    install_port net openmpi
	fi
    fi
}


slurm_config()
{
    # FIXME: creating these directories should not be necessary
    # --syslog?
    mkdir -p /var/log/munge /var/run/munge /var/lib/munge
    
    # Generate munge.key
    $LOCALBASE/etc/rc.d/munged stop > /dev/null 2>&1 || true
    munge_dir="$LOCALBASE/etc/munge"
    if [ $NODE_TYPE == 'head' ]; then
	if [ ! -e $munge_dir/munge.key ]; then
	    mkdir -p -m 0700 $munge_dir
	    touch $munge_dir/munge.key
	    chmod 400 $munge_dir/munge.key
	    printf "Generating munge key...\n"
	    dd if=/dev/random bs=1 count=1024 > $munge_dir/munge.key
	fi
    else
	# Could use munge_flags="--key-file $HEAD_USR/local/etc/munge/munge.key"
	if [ ! -e $munge_dir/munge.key ]; then
	    mkdir -p -m 0700 $munge_dir
	    scp -p ${head_host}:$munge_dir/munge.key $munge_dir
	    pause
	fi
    fi
    
    # Enable munge
    auto-enable-service munged $0 > $LOG_DIR/munged_enable 2>&1
    service munged restart

    slurm_conf="$LOCALBASE/etc/slurm.conf"
    if [ $NODE_TYPE = 'head' ]; then
	line
	cat << EOM

After you press return, slurm.conf will be opened in $EDITOR.

You can also generate a SLURM config file using the web wizard at:

    $LOCALBASE/share/doc/slurm-*/html/configurator.html

The slurmctld daemon will be started after you exit the editor.

EOM
	line
	pause
	if [ ! -e $slurm_conf ]; then
	    cp $slurm_conf.example $slurm_conf
	fi
	$EDITOR $slurm_conf
	stop_service slurmctld
	auto-enable-service slurmctld $0 > $LOG_DIR/slurmctld_enable 2>&1
	service slurmctld restart
    else
	scp ${head_host}:$slurm_conf $slurm_conf
	service slurmd stop > /dev/null 2>&1 || true
	auto-enable-service slurmd $0 > $LOG_DIR/slurmd_enable 2>&1
	service slurmd restart
    fi
}


scheduler_config()
{
    line
    if auto-package-installed sysutils/slurm-hpc; then
	slurm_config
    fi
    
    if auto-package-installed sysutils/condor; then
	if [ ! -e $LOCALBASE/etc/condor_config ]; then
	    cp $LOCALBASE/etc/condor/condor_config $LOCALBASE/etc
	fi
	if [ ! -e $LOCALBASE/etc/condor_config.local ]; then
	    cp $LOCALBASE/etc/condor/condor_config.local $LOCALBASE/etc
	fi
	$EDITOR /usr/local/etc/condor_config*
    fi
    
    # SGE is reinstalled as a dependency of openmpi
    if auto-package-installed sysutils/sge62; then
	sge_config      # Run SGE setup tools
	
	printf "\nUpdating /etc/services for SGE...\n"
	# Check /etc/services
	if ! fgrep -q sge_qmaster /etc/services; then
	    printf "sge_qmaster     6444/tcp   #Grid Engine Qmaster Service\n" >> /etc/services
	    printf "sge_qmaster     6444/udp   #Grid Engine Qmaster Service\n" >> /etc/services
	fi
	if ! fgrep -q sge_qmaster /etc/services; then
	    printf "sge_execd       6445/tcp   #Grid Engine Execution Service\n" >> /etc/services
	    printf "sge_execd       6445/udp   #Grid Engine Execution Service\n" >> /etc/services
	fi
	
	# Look into this later...
	# Install PVM utilities if desired
	#printf "Install SGE PVM utils? (y/n) "
	#read resp
	#if [ 0$resp = 0'y' ]; then
	#    cd $LOCALBASE/sge/pvm/src
	#    ./aimk
	#    ./install.sh
	#    cd $start_dir
	#fi
    fi
    
    if auto-package-installed sysutils/torque; then
	if [ ! -e $TSPOOL ]; then
	    cp -Rp $LOCALBASE/share/examples/torque$TSPOOL $TSPOOL
	    chmod 1777 $TSPOOL/checkpoint $TSPOOL/spool $TSPOOL/undelivered
	fi
	
	# Package missing arrays dir
	mkdir -p $TSPOOL/server_priv/arrays
	
	# On compute nodes, put '$pbsserver ip.address.of.server' into
	# $TSPOOL/mom_priv/config
	
	##################################################################
	# Head node (torque server) only
	#
	
	if [ $NODE_TYPE = "head" ]; then
	    # Use main interface address on server if there are multiple
	    # interfaces, not the local interface!  Torque will get
	    # confused otherwise.
	    # Also use short hostname, which must be in /etc/hosts.
	    # If someone messes up /etc/hosts, we prefer a clear lookup
	    # failure over getting the wrong address from DNS.
	    hostname -s > $TSPOOL/server_name
    
	    # Cited as useful for dual nic servers, but not needed
	    #printf "SERVERHOST\t$head_host\n" > $TSPOOL/torque.cfg
	    #$EDITOR $TSPOOL/torque.cfg
    
	    # To configure queues on server, execute
	    # $LOCALBASE/share/examples/torque/torque.setup <admin>
	    stop_pbs

	    # Annihilates server_priv/nodes.  Edit nodes AFTER this!
	    #if [ ! -e $TSPOOL ]; then
		printf "Running torque.setup...\n"
		printf "y\n" | $LOCALBASE/share/examples/torque/torque.setup root \
		    > $LOG_DIR/torque.setup
		sleep 5
		stop_pbs
	    #fi
    
	    line
	    nodes_file=$TSPOOL/server_priv/nodes
	    nodes_backup=$LOCALBASE/cluster/server_priv-nodes
	    if [ -e $nodes_backup ]; then
		cp $nodes_backup $nodes_file
	    fi
	    cat << EOM

Editing $nodes_file...

Add names of existing compute nodes and options.  Compute nodes that are
not yet operational should not be added at this time.

EOM
	    printf "Press enter when ready to enter the editor..."
	    read junk
	    $EDITOR $nodes_file
	    cp $nodes_file $nodes_backup

	    # auto-enable-service doesn't start services that are already
	    # configured in rc.conf
	    auto-enable-service pbs_server cluster-setup \
		> $LOG_DIR/pbs_server_enable 2>&1
	    sleep 2
	    service pbs_server restart 2> $LOG_DIR/pbs_server_start
	    sleep 2

	    auto-enable-service pbs_sched cluster-setup 
		> $LOG_DIR/pbs_sched_enable 2>&1
	    sleep 2
	    service pbs_sched restart 2> $LOG_DIR/pbs_sched_start
	    sleep 2
	else
	    # Use local address of head node on compute nodes.  If
	    # head node has multiple interfaces, the DNS entry will
	    # provide the public interface IP rather than the private
	    # cluster IP.
	    # Also use short hostname, which must be in /etc/hosts.
	    # If someone messes up /etc/hosts, we prefer a clear lookup
	    # failure over getting the wrong address from DNS.
	    printf "$head_host\n" > $TSPOOL/server_name
    
	    line
	    #printf "\nAdd `hostname -s` to server_priv/nodes on the server\n"
	    #printf "and restart pbs_server.\n\n"
	    #pause
	    
	    printf "Adding $(hostname -s) to server_priv/nodes on $head_host\n"
	    ssh $head_host auto-append-line "$(hostname -s)" \
		"$(hostname -s)" $TSPOOL/server_priv/nodes \
		cluster-setup
	    ssh $head_host service pbs_server restart
	    
	    if fgrep -q pbs_mom $RC_CONF; then
		if ! service pbs_mom stop > /dev/null 2>&1; then
		    if killall pbs_mom > /dev/null 2>&1; then
			printf "Old pbs_mom terminated.\n"
		    fi
		fi
	    fi

	    # On compute nodes, add the following line to $RC_CONF:
	    # pbs_mom_enable="YES"
	    auto-enable-service pbs_mom cluster-setup \
		> $LOG_DIR/pbs_mom_enable 2>&1
	    sleep 5
	    service pbs_mom restart
	    ssh $head_host pbsnodes
	    # printf "\nTest this node by running 'pbsnodes' on head node.\n\n"
	    pause
	fi
	
	# Change torque/spool and torque/undelivered to 777?
	# Does not appear necessary anymore.
    fi
}


# Currently must come after Torque setup.  Reorg to consolidate scheduler conf
nfs_config()
{
    df
    cat << EOM

Some schedulers require a shared spool or config directory.  Check the
documentation for your scheduler before deciding which directories to
share via NFS.

Some other directories you may want to share include:

    /usr/home (or /home)
    /var/cache/pkg
    /usr/ports/packages

EOM

    line
    nfs_config=`auto-ask nfs-config 'Configure NFS?' n`
    if [ $nfs_config != 'y' ]; then
	return
    fi

    auto-enable-service -s statd rpc_statd $0
    auto-enable-service -s lockd rpc_lockd $0
    
    case $NODE_TYPE in
    "head")
	# Configure NFS whether or not compute nodes will use it
	# Export /usr (which should include /usr/home)
	# FIXME: Make sure NFS server is enabled in rc.conf first
	# Does not work for multi-homed host
	# ip=`fgrep $head_host /etc/hosts | awk ' { print $1 }' | uniq`
	subnet=`auto-ask subnet "Subnet for NFS?" 10.1.0.0`
	
	share='x'
	while [ 0$share != 0 ]; do
	    df
	    printf "Directory to export? [Press enter to quit] "
	    read share
	    
	    if [ 0$share != 0 ]; then
		# FIXME: Add support for ZFS
		fs_type=`mount | awk '$3 == "/usr/home" { print $4 }'`
		if [ $fs_type == '(zfs,' ]; then
		    zfs_share=`mount | awk '$3 == "/usr/home" { print $1 }'`
		    zfs set sharenfs="-maproot=root -network $subnet -mask $NETMASK" $zfs_share
		else
		    printf "Updating /etc/exports...\n"
		    auto-append-line "$share\t-maproot=0\t-network $subnet -mask $NETMASK\n" /etc/exports $0
		fi
	    fi
	done
	zfs get sharenfs
	
	printf "Updating $RC_CONF...\n"
	# Need UDP for some PXE ROMs
	auto-append-line 'nfs_server_flags="-t -u -n 16"' $RC_CONF $0
	auto-enable-service nfs_server $0 > $LOG_DIR/nfsd_enable 2>&1
	service nfsd restart
	killall -HUP mountd
	printf "\nYou may need to reboot before NFS clients can mount the new shared folder.\n\n"
	pause
	;;
	
    "compute")

	host='x'
	while [ 0$host != 0 ]; do
	    printf "\nHost name of NFS server? [Press enter to quit] "
	    read host
	    
	    if [ 0$host != 0 ]; then
		printf "Directory to mount from $host? "
		read share
		
		printf "Local mount point? [$share] "
		read mount
		if [ 0$mount = 0 ]; then
		    mount=$share
		fi
		
		# Unmount local if necessary
		# First try ZFS
		zfs_ds=`zfs list | awk -v share=$share '$5 == share { print $1 }'`
		if [ 0$zfs_ds != 0 ]; then
		    zfs set mountpoint=none $zfs_ds
		fi
		# Not a ZFS mount?  Maybe some other FS.
		if mount | fgrep $share; then
		    umount $share
		fi
		
		# Configure shared directory
		found=`awk '$1 == "'${head_host}:$share'" { print $1 }' /etc/fstab`
		if [ 0$found = 0 ]; then
		    printf "\nUpdating /etc/fstab...\n\n"
		    printf "# Generated by $0.\n" >> /etc/fstab
		    printf "# Adjust rsize and wsize if needed to improve performance.\n" >> /etc/fstab
		    printf "${head_host}:$share\t\t$mount\tnfs\trw,intr,noatime,rsize=8192,wsize=8192\t0\t0\n" >> /etc/fstab
		else
		    printf "Remote filesystem $head_host:$share already configured.\n"
		fi
		mkdir -p $share
		mount $share
	    fi
	done
    
	# FIXME: Find a way to list torque nodes that doesn't depend on
	# a specific NFS mount.
	# Add name of node to master list
	#if auto-package-installed sysutils/torque; then
	#    add_node_name `hostname -s` $HEAD_USR/local/cluster/torque_nodes
	#fi
	;;
	
    *)
	;;
    esac
}


# Must come after NFS setup
sge_config()
{
    case $NODE_TYPE in
    "head")
	# Configure SGE master
	if auto-package-installed sysutils/sge62; then
	    #cd $LOCALBASE/sge
	    # -csp is for added security
	    #./install_qmaster -csp
	    #cd $start_dir
	    cell=`auto-ask sge-cell "SGE cell name [default='default']?" default`
	    if [ 0$cell = 0 ]; then
		cell='default'
	    fi
	    
	    if ! fgrep -q "sge_cell" $RC_CONF; then
		printf "sge_cell=\"$cell\"\n" >> $RC_CONF
	    fi
	    if ! fgrep -q "sge_qmaster" $RC_CONF; then
		printf "sge_qmaster_enable=\"YES\"\n" >> $RC_CONF
	    fi
	    
	    if [ ! -e $LOCALBASE/sge/$cell ]; then
		printf "\nRunning $LOCALBASE/sge/install_qmaster...\n\n"
		pause
		save_cwd=`pwd`
		cd $LOCALBASE/sge
		./install_qmaster
		cd $save_cwd
	    fi
	fi
	;;
    compute)
	# Configure SGE node
	if auto-package-installed sysutils/sge62; then
	    #cd $LOCALBASE/sge
	    #./install_execd
	    #cd $start_dir
	    
	    cell=`auto-ask sge-cell "SGE cell name [default='default']?" default`
	    if [ 0$cell = 0 ]; then
		cell='default'
	    fi
	    if ! fgrep -q "sge_cell" $RC_CONF; then
		printf "sge_cell=\"$cell\"\n" >> $RC_CONF
	    fi
	    if ! fgrep -q "sge_execd" $RC_CONF; then
		printf "sge_execd_enable=\"YES\"\n" >> $RC_CONF
	    fi
	    
	    # FIXME: Don't expect HEAD_USR - let user choose mounts
	    # Link cell directory or whole SGE directory from head to
	    # $LOCALBASE/sge
	    #ln -sf $HEAD_USR/local/sge/$cell $LOCALBASE/sge
	    
	    # 
	    cat << EOM
	
Ready to run SGE setup tool...
Make sure $(hostname) is configured as an administrative host
On head node: Run qconf -sh to check and qconf -ah to add.

EOM
	    pause
    
	    #if [ ! -e $LOCALBASE/sge/$cell ]; then
		printf "\nRunning $LOCALBASE/sge/install_execd...\n\n"
		pause
		save_cwd=`pwd`
		cd $LOCALBASE/sge
		./install_execd
		cd $save_cwd
	    #fi
	fi
	;;
    esac
}


ganglia_config()
{
    ##########################################################################
    #   Head node: bind in both send and receive?
    #   Compute nodes: host = head IP
    ##########################################################################
    
    if [ $NODE_TYPE = "head" ]; then
	# Install apache
	line
	printf "Configuring apache...\n"
	if ! auto-package-installed www/$APACHE_PKG; then
	    install_packages www/$APACHE_PKG
	    pause
	fi

	if [ ! -e $LOCALBASE/etc/$APACHE_PKG/httpd.conf.orig ]; then
	    printf "Patching httpd.conf...\n"
	    mv $LOCALBASE/etc/$APACHE_PKG/httpd.conf \
		$LOCALBASE/etc/$APACHE_PKG/httpd.conf.orig
	    awk -f $DATADIR/patch-apache.awk \
		$LOCALBASE/etc/$APACHE_PKG/httpd.conf.orig \
		> $LOCALBASE/etc/$APACHE_PKG/httpd.conf
	fi
	
	# Install a rudimentary home page
	if [ ! -e $LOCALBASE/www/$APACHE_PKG/data/global_styles.css ]; then
	    cp $DATADIR/WWW/* $LOCALBASE/www/$APACHE_PKG/data
	    sed -i '' -e "s|%%HOSTNAME%%|$(hostname)|g" \
		$LOCALBASE/www/$APACHE_PKG/data/index.php
	fi
	
	# Check for php5 apache module.  It is not included in the
	# standard php5 build. Need WITH_APACHE.
	install_packages lang/php5 www/mod_php5
	
	# FIXME: Dying here on first run
	# Must come after php5 install, since http.conf was patched
	auto-enable-service $APACHE_PKG cluster-setup \
	    > $LOG_DIR/${APACHE_PKG}_enable 2>&1
	service $APACHE_PKG reload
	
	# Add date_default_timezone_set('America/Chicago'); to ganglia.php
	# or update date.timezone in $LOCALBASE/etc/php.ini and restart
	# Apache
	if [ ! -e $LOCALBASE/etc/php.ini ]; then
	    cp $LOCALBASE/etc/php.ini-production $LOCALBASE/etc/php.ini
	fi
	
	# Set timezone if not already set
	if ! grep -q '^date.timezone' $LOCALBASE/etc/php.ini; then
	    printf "Time zone? (Example: America/Chicago) "
	    # FIXME: Find a way to validate input
	    read zone
	    sed -i '.orig' -e "s|;date.timezone =|date.timezone = $zone|g" \
		$LOCALBASE/etc/php.ini
	fi
	
	line
	# Install ganglia web frontend
	if ! auto-package-installed sysutils/ganglia-web; then
	    install_packages sysutils/ganglia-web
	fi

	# patch creates gmetad.orig
	sed -i '' \
	    -e 's|my cluster|all nodes|g' \
	    -e 's|# gridname "MyGrid"|gridname "%%SHORT_HOSTNAME%%"|g' \
	    -e 's|# trusted_hosts 127.0.0.1 169.229.50.165 my.gmetad.org|trusted_hosts 127.0.0.1 %%LOCAL_IP%% %%LONG_HOSTNAME%%|g' \
	    $LOCALBASE/etc/gmetad.conf
	
	# Configure ganglia
	if grep -q '%%SHORT_HOSTNAME%%' $LOCALBASE/etc/gmetad.conf; then
	    # FIXME: gmetad crashes if COMPUTE_NODE_LIST is too long
	    sed -i '' -e "s|%%SHORT_HOSTNAME%%|$(hostname -s)|g" \
		      -e "s|%%LONG_HOSTNAME%%|$(hostname)|g" \
		      -e "s|%%LOCAL_IP%%|$HEAD_IP|g" \
		$LOCALBASE/etc/gmetad.conf
	fi

	line
	stop_service $APACHE_PKG
	service $APACHE_PKG restart
    else
	line
	cat << EOM
If your head node is serving as a gateway, you may need to add
$(hostname -s) to the data_source line in 
${head_host}:/usr/local/etc/gmetad.conf.
EOM
	pause
    fi
    
    line
    if ! auto-package-installed sysutils/ganglia-monitor-core; then
	install_packages sysutils/ganglia-monitor-core
    fi
    
    if grep -q 'name = "unspecified"' $LOCALBASE/etc/gmond.conf; then
	if [ $NODE_TYPE = head ]; then
	    name=$(hostname -s)
	else
	    name=$head_host
	fi
	sed -i '.orig' \
	    -e "s|name = \"unspecified\"|name = \"$name\"|g" \
	    $LOCALBASE/etc/gmond.conf
    fi
    
    
    stop_service gmond
    auto-enable-service gmond cluster-setup \
	> $LOG_DIR/gmond_enable 2>&1
    service gmond restart
    
    if [ $NODE_TYPE = "head" ]; then
	stop_service gmetad
	auto-enable-service gmetad cluster-setup \
	    > $LOG_DIR/gmetad_enable 2>&1
	service gmetad restart
    fi
    
    line
    cat << EOM
    Test this node by browsing to http://$head_host/ganglia/.
    
    Note that it may take a minute or so for new nodes to appear.
EOM
    pause
}


##########################################################################
#   Function description:
#       Add this host to authorized_hosts on remote host
#
#   Arguments:
#       1) remote host
#       
#   History:
#   Date        Name        Modification
#   2013-03-01  Charlie &   Begin
##########################################################################

ssh_authorize()
{
    if [ $# -lt 1 ] ; then
       echo "usage: ssh-authorize remotehost"
       exit 1
    fi
    remotehost=$1
    
    cd
    
    user=`whoami`
    echo "Authorizing ${user}@${HOST} on $remotehost."
    ssh $remotehost 'umask 077; mkdir -p .ssh'

    # See if key already exists on remote host
    if ssh $remotehost "grep -q ${user}@${HOST} .ssh/authorized_keys"; then
	printf "Key already exists for this host.\n"
	return
    fi
    
    #
    # If a key hasn't been generated then do it.
    #
    if [ ! -f .ssh/id_dsa.pub ] ; then
	printf "Error: ssh_authorize must be called AFTER id_dsa is installed.\n"
	exit 1
    fi
    
    key=`cat .ssh/id_dsa.pub`
    ssh $remotehost "echo $key >> .ssh/authorized_keys ; chmod 600 .ssh/authorized_keys"
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

ssh_config()
{
    # Verify that nfs_config has been run

    line
    printf "Configuring ssh...\n"
    
    # Permit root login over ssh
    if ! grep -iq '^PermitRootLogin without-password' /etc/ssh/sshd_config; then
	sed -i ".bak" 's|#PermitRootLogin no|PermitRootLogin without-password|g' /etc/ssh/sshd_config
	# killall -HUP sshd
	service sshd reload
	line
	cat << EOM

WARNING: 'PermitRootLogin without-password' has been enabled.

This is necessary for clusters that use keyed SSH between nodes for system
activities.  If you are concerned about security, you can manually
reconfigure the head node.

EOM
    fi
    
    # All nodes should already accept passwordless login from the head node
    # This is set up by auto-pxe-installer-setup and left to the user
    # is they are not using it.
    if [ $NODE_TYPE = 'head' ]; then
	# Disable prompting for new hosts, etc.
	cluster-update-ssh_config
    else
	# Add this host to authorized keys for root on head node(s)
	auto-ssh-authorize $head_host
    fi
}


##########################################################################
#   Function description:
#       Must be done after NFS config due to use of $HEAD_USR.
#
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

resource_limits()
{
    # Set resource limits
    line
    printf "Configuring resource limits...\n"
    if [ ! -e /etc/login.conf.orig ]; then
	cp /etc/login.conf /etc/login.conf.orig
    fi

    if grep -q maxproc=unlimited /etc/login.conf \
	|| grep -q vmemoryuse=unlimited /etc/login.conf; then
	cat << EOM
Setting desired resource limits in /etc/login.conf.

It is a good idea to limit vmemoryuse and maxproc on the head node
to prevent it from being overloaded by careless users.

The scheduler should manage limits on compute nodes, but some generous hard
limits on vmemoryuse and maxproc in login.conf can help protect nodes from
fork() bombs and memory leaks that the scheduler does not catch (fast enough).
Usually, something like 10 + the number of cores is a good limit for compute
nodes.

EOM
	default_maxproc=$(( `sysctl -n kern.smp.cpus` * 2 ))
	maxproc=`auto-ask max-proc "Max processes for the $NODE_TYPE node?" $default_maxproc`
	
	default_vmem=$(( `sysctl -n hw.realmem` / 1024 / 1024 ))m
	vmem=`auto-ask vmem "Max memory use for the $NODE_TYPE node? (use 'm' or 'g' suffix)" $default_vmem`
	
	umask=`auto-ask umask "umask for the $NODE_TYPE node?" 027`
	sed -i '' -e "s/maxproc=unlimited/maxproc=$maxproc/g" \
		 -e "s/vmemoryuse=unlimited/vmemoryuse=$vmem/g" \
		 -e "s|umask=022|umask=$umask|g" \
		 /etc/login.conf
	$EDITOR /etc/login.conf
	cap_mkdb /etc/login.conf
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-02-15  Charlie &   Begin
##########################################################################

update_system()
{
    line
    printf "Updating system clock...\n"
    stop_service ntpd
    if killall -9 ntpd > /dev/null 2>&1; then
	printf "Killed rogue ntp.\n"
    fi
    ntpdate -u pool.ntp.org
    auto-enable-service ntpd cluster-setup > /dev/null 2>&1
    service ntpd restart
    
    freebsd_update=`auto-ask freebsd-update 'Update system base?' y`
    if [ $freebsd_update = y ]; then
	freebsd-update fetch install
    fi
    
    portsnap=`auto-ask portsnap 'Update ports tree?' y`
    if [ $portsnap = y ]; then
	portsnap fetch
	if [ -e /usr/ports ]; then
	    portsnap update
	else
	    portsnap extract
	fi
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

install_common_tools()
{
    # Quick-install some common tools
    line
    install_packages devel/gmake net/rsync converters/libiconv \
	editors/nano ftp/wget devel/subversion
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

get_port_install_method()
{
    line
    resp=`auto-ask use-source "Build ports from source?" n`
    if [ 0$resp = 0'y' ]; then
	export AUTO_BUILD_FROM_SOURCE='yes'
    else
	export AUTO_BUILD_FROM_SOURCE='fall-back'
	if [ x$AUTO_PACKAGEROOT = x ]; then
	    printf "Finding fastest mirror... "
	    export AUTO_PACKAGEROOT=`auto-fastest-mirror`
	    printf "$AUTO_PACKAGEROOT\n"
	fi
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

startup_scripts()
{
    # Install extra login logic
    line
    printf "Installing startup scripts...\n"
    if [ ! -e $LOCALBASE/cluster/profile ]; then
	cp $DATADIR/profile $LOCALBASE/cluster
    fi
    if [ ! -e $LOCALBASE/cluster/csh.login ]; then
	cp $DATADIR/csh.login $LOCALBASE/cluster
    fi
    auto-append-line cluster/profile ". $LOCALBASE/cluster/profile" /etc/profile cluster-setup
    auto-append-line cluster/csh.login "source $LOCALBASE/cluster/csh.login" /etc/csh.login cluster-setup
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

enable_procfs()
{
    line
    auto-enable-procfs
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

reboot_maybe()
{
    line
    cat << EOM

You must reboot to test the new configuration.  If this is the first time
you completed the cluster-setup $NODE_TYPE process, you should reboot now.

EOM

    printf "Reboot? (y/[n]) "
    read reboot
    if [ 0$reboot = 0y ]; then
	shutdown -r now
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2014-05-15  Charlie &   Begin
##########################################################################

verify_network()
{
    cat << EOM

Before proceeding:

1)  Confirm that your IP and netmask are correct in /etc/rc.conf.

2)  Confirm that /etc/hosts contains a correct entry for each interface.

EOM
    pause
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2015-01-30  Charlie &   Begin
##########################################################################

get_head_host()
{
    printf 'Make sure /etc/hosts is up-to-date.\n'
    pause
    
    if [ $NODE_TYPE = "compute" ]; then
	# If cluster-setup was previously completed on this node,
	# get head node from configuration, otherwise ask.
	head_host=''
	while [ 0$head_host = 0 ]; do
	    head_host=`auto-ask head-hostname "Short hostname of head node?" login`
	done
    else
	head_host=`hostname -s`
	hostname -s > $LOCALBASE/cluster/head_node
    fi
}


##########################################################################
#   Main
##########################################################################

if [ $# != 1 ]; then
    usage $0
fi

case $1 in
head|compute)
    ;;
*)
    usage $0
    ;;
esac

if [ `whoami` != root ]; then
    printf "$0 must be run as root.\n"
    exit 1
fi

if [ ! -e /root/ssh.backup ]; then
    printf 'Backing up .ssh...\n'
    cp -R /root/.ssh /root/ssh.backup
fi

##########################################################################
#   Set common variables
##########################################################################

EDITOR=`auto-ask editor 'What is your preferred editor?' $EDITOR`
export EDITOR

$EDITOR /etc/hosts.allow

# Recommended layout for dedicated hardware:
# Use netmask of 255.255.0.0.
# Head node is 10.1.1.1 if it's a gateway or 10.1.1.2 with a separate router.
# 10.1.1.3 - 10.1.1.254 are for extra head nodes, I/O nodes, etc.
# Compute nodes begin at 10.1.2.1
MAX_NODES=60000

# Misc
LOCALBASE="/usr/local"
RC_CONF='/etc/rc.conf'
START_DIR=`pwd`
APACHE_PKG=apache24

# Used by child scripts, so export it
LOG_DIR=$START_DIR/cluster-setup-log
line
printf "Command output will be stored in $LOG_DIR.\n"
line
pause

# Source for default files
DATADIR=$LOCALBASE/share/cluster-admin

TSPOOL="/var/spool/torque"

NODE_TYPE=$1
case $NODE_TYPE in
"head"|"compute")
    ;;
*)
    printf "Invalid installation type: $NODE_TYPE\n"
    usage $0
esac

# Config files used by cluster-admin
if [ $NODE_TYPE = head ]; then
    CLUSTER_DATA="$LOCALBASE/cluster"
    mkdir -p $CLUSTER_DATA
fi

verify_network
HEAD_IP=`auto-ask head-ip 'IP address of head node LOCAL interface?' 10.1.1.2`
NETMASK=`auto-ask netmask 'Netmask for local network?' 255.255.0.0`

##########################################################################
#   Begin setup
##########################################################################

# The following configuration must be done on all nodes, although
# different node types may be configured differently.
update_system               # Start with and NTP update
enable_procfs               # Needed for?
get_port_install_method     # Build ports from source?
install_common_tools        # gmake, etc.
get_head_host
install_scheduler           # Select and install a scheduler
nfs_config                  # Set up head node NFS server.
ssh_config                  # Enable keys between nodes. After nfs_config.
scheduler_config            # Configure scheduler.  After ssh_config.
resource_limits             # Configure login.conf.  After nfs_config.
ganglia_config              # Cluster monitoring.

# Configuration steps that only occur on certain node types
case $NODE_TYPE in
"head")
    startup_scripts
    cluster-update-all-ssh_config
    ;;

"compute")
    # Install after scheduler.  OpenMPI port has options for scheduler
    # integration.
    install_mpi
    ssh $head_host cluster-update-ssh_config
    cluster-update-ssh_config
    ;;
*)
    ;;
esac

# Reboot to test new config after initial setup
reboot_maybe

