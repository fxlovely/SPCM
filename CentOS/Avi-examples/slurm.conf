#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName=Avi
ControlMachine=login
#ControlAddr=
BackupController=login2
#BackupAddr=
#
# Causes
# Uncomment next time scheduler is restarted
# Notify users of the change before doing this
DefMemPerCPU=256
# FIXME: We're getting false indicatins of mismatched slurm.conf on
# compute nodes.  This is to silence them until the problem is fixed.
MessageTimeout=30
DebugFlags=NO_CONF_HASH
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
# Must be readable and writable by both primary and backup controller daemons
StateSaveLocation=/home/slurm/slurmctld
# Must be the same pathname on all nodes, but a local directory
SlurmdSpoolDir=/var/spool/slurmd
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
# Slow
# ProctrackType=proctrack/pgid
Proctracktype=proctrack/linuxproc
MailProg=/bin/mailx
#PluginDir=
CacheGroups=0
#FirstJobId=
ReturnToService=1
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
PropagateResourceLimitsExcept=ALL
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
#
# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
# cons_res schedules cores as resources, linear uses whole nodes
SelectType=select/cons_res
#SelectType=select/linear
FastSchedule=1
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
#
# LOGGING
SlurmctldDebug=4
SlurmctldLogFile=/var/log/slurm/slurmctld
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd
JobCompType=jobcomp/filetxt
JobCompLoc=/home/slurm/Jobcomp
#
# ACCOUNTING
JobAcctGatherType=jobacct_gather/linux
JobAcctGatherFrequency=30
#
#AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageType=accounting_storage/filetxt
#AccountingStorageHost=
AccountingStorageLoc=/home/slurm/Accounting
#AccountingStoragePass=
#AccountingStorageUser=
#
# COMPUTE NODES
# FIXME: Problems with vasp spawning too many procs
# NodeName=compute-1-[01-36],compute-2-[01-36],compute-3-[01-20],compute-4-[01-14],compute-5-[01-36] Sockets=2 CoresPerSocket=4 ThreadsPerCore=2 State=UNKNOWN

# Switch to this next time the scheduler is restarted
# DownNodes= Reason="" State=DOWN|DRAIN
NodeName=compute-1-[01-36],compute-2-[01-36],compute-3-[01-20],compute-4-[01-14],compute-5-[01-34] RealMemory=24000 CPUS=8 State=UNKNOWN
NodeName=compute-5-[35-36] RealMemory=128000 CPUS=8 State=UNKNOWN
PartitionName=batch Nodes=compute-1-[01-36],compute-2-[01-36],compute-3-[01-20],compute-4-[01-14],compute-5-[01-34] Default=YES MaxTime=INFINITE MaxMemPerNode=24000 State=UP
PartitionName=highmem Nodes=compute-5-[35-36] Default=NO MaxTime=INFINITE MaxMemPerNode=128000 State=UP
