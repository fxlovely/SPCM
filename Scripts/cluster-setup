#!/bin/sh -e

##########################################################################
#   Description:
#       Automate setup of a simple FreeBSD cluster.
#       Installs common software and configures settings to facilitate
#       cluster operation.  Assumes a single head node will handle
#       job initiation/scheduling and act as a file server for all nodes.
#
#   Usage:
#       First run 
#
#           cluster-setup head
#
#       on the head node.  Then run
#
#           cluster-setup compute
#
#       on the rest.
#       
#   History:
#       Dec 2009    J Bacon
##########################################################################


usage()
{
    printf "Usage: $0 head|compute\n"
    exit 1
}


line()
{
    printf "==============================================================================\n"
}


pause()
{
    local junk
    printf "Press return to continue..."
    read junk
}


stop_pbs()
{
    printf "Stopping PBS daemons...\n"
    if fgrep -q pbs_sched_enable /etc/rc.conf; then
	if /usr/local/etc/rc.d/pbs_sched stop 2> /dev/null; then
	    printf "Stopped pbs_sched.\n"
	fi
	sleep 2
    fi
    if fgrep -q pbs_server_enable /etc/rc.conf; then
	if /usr/local/etc/rc.d/pbs_server stop 2> /dev/null; then
	    printf "Stopped pbs_server.\n"
	fi
	sleep 2
    fi
    if killall pbs_sched 2> /dev/null; then
	printf "Rogue pbs_sched terminated.\n"
	sleep 2
    fi
    if killall pbs_server 2> /dev/null; then
	printf "Rogue pbs_server terminated.\n"
	sleep 2
    fi
}


##########################################################################
#   Description:
#       Run command given by arguments and log output.
##########################################################################

log_cmd()
{
    if [ $# -lt 1 ]; then
	printf "Usage: $0 command [args]\n"
	exit 1
    fi
    log_dir=$START_DIR/desktop-installer-log
    if [ ! -e $log_dir ]; then
	mkdir $log_dir
    fi
    $* 2>&1 | tee -a $log_dir/$1.log
}


##########################################################################
#   Description:
#       Install ports or packages specified by arguments if they are not
#       already installed.  If $USE_PORTS is true, installs from source,
#       otherwise attempts to install using pkg_add.
#
#   Arguments:
#       List of ports in the for "category/name"
##########################################################################

install_packages()
{
    for pkg in $*; do
	log_cmd auto-install-packages $pkg
    done
}


install_port()
{
    local resp category port
    
    if [ $# -lt 2 ]; then
	printf "Usage: install_port category port [make flags]\n"
	exit 1
    fi
    
    category=$1
    port=$2
    shift
    shift
    # FIXME: Use cluster-pkgdir
    if [ -e $head_usr/ports/packages/All ] && \
	ls $head_usr/ports/packages/All/${port}* ; then
	printf "\nInstalling $port from package...\n\n"
	pause
	pkg_add $head_usr/ports/packages/All/${port}*
    else
	printf "\nInstalling $port from port...\n\n"
	pause
	cd /usr/ports/$category/$port
	make deinstall
	printf "Clean port and rebuild? (y/n) [n] "
	read resp
	if [ 0$resp = 0'y' ]; then
	    make clean
	    make rmconfig
	fi
	make reinstall
    fi
}


add_node_name()
{
    if [ $# != 2 ]; then
	printf "Usage: $0 node-name node-file\n"
	exit 1
    fi
    
    name=$1
    file=$2
    
    # Prevent fgrep from failing
    if [ ! -e $file ]; then
	touch $file
    fi
    
    if ! fgrep -q $name $file; then
	printf "$name\n" >> $file
    fi
}


edit_hosts()
{
    line
    printf "Generating /etc/hosts file...\n"
    if [ $node_type = "compute" ]; then
	# If cluster-setup was previously completed on this node,
	# get head node from configuration, otherwise ask.
	if [ -e $head_usr/local/cluster/head_node ]; then
	    head_host=`cat $head_usr/local/cluster/head_node`
	else
	    head_host=''
	    while [ 0$head_host = 0 ]; do
		head_host=`auto-ask head-hostname "Short hostname of head node?" unknown`
	    done
	fi
	head_host_entry="192.168.0.2\t\t$head_host.local $head_host\n"
    else
	head_host=`hostname -s`
	hostname -s > $PREFIX/cluster/head_node
	head_host_entry="192.168.0.2\t\t$head_host.local\n"
    fi
    
    if ! fgrep -q 'Entries generated by cluster-setup' /etc/hosts; then
	printf "\n# Entries generated by cluster-setup\n" >> /etc/hosts
	printf "$head_host_entry" >> /etc/hosts
	node=1
	while [ $node -lt $MAX_NODES ]; do
	    ip=$((node+10))
	    printf "192.168.0.%d\t\tcompute-%03d.local compute-%03d\n" \
		$ip $node $node >> /etc/hosts
	    node=$((node+1))
	done
    fi
}


install_scheduler()
{
    line
    # Patch bug in openmpi run_depends
    sed -i '.bak' 's|${SGE_ROOT}/fbsd-${ARCH}/qrsh|${SGE_ROOT}/bin/fbsd-${ARCH}/qrsh|g' \
	/usr/ports/net/openmpi/Makefile
    
    cat << EOM
Please choose a scheduler:

1.. SGE (Requires X11)
2.. Torque PBS

EOM

    resp=' '
    while [ 0$resp != 01 ] && [ 0$resp != 02 ]; do
	resp=`auto-ask scheduler "Enter 1 or 2" unknown`
	case $resp in
	1)
	    if ! auto-package-installed sysutils/sge62; then
		install_packages sysutils/sge62
	    
		line
		printf "You can ignore the post-install messages above.\n"
		printf "The services will be configured automatically.\n"
		pause
		
		# Source $PREFIX/sge/default/common/settings.csh
		# All execution hosts must be administrative hosts during the installation.
		# All hosts which you added to the list of administrative hosts during this
		# installation procedure can now be installed.
		line
		printf "\nAll users should source $PREFIX/sge/<cell>/common/settings.[c]sh\n\n"
		pause
	    else
		printf "SGE is already installed.\n"
	    fi
	    ;;
	2)
	    if ! auto-package-installed sysutils/torque &&
		! auto-package-installed sysutils/torque2; then
		printf "Installing torque from package...\n"
		torque_version=' '
		while [ 0$torque_version != 02 ] && \
		      [ 0$torque_version != 03 ]; do
		    printf "Torque version 2 or 3? "
		    read torque_version
		done
		if [ $torque_version == 3 ]; then
		    install_packages sysutils/torque devel/gmake
		else
		    install_packages sysutils/torque2 devel/gmake
		fi
		# Package does not set permissions on spool directories
		# Also, users are encouraged to update sysutils/torque
		# to the latest version instead of using the package
		pkg_delete /var/db/pkg/torque*
		auto-install-packages lang/perl devel/libtool
		printf "Reinstalling torque from port...\n"
		install_port sysutils torque
	    
		line
		printf "You can ignore the post-install messages above.\n"
		printf "The services will be configured automatically.\n"
		pause
	    else
		printf "Torque is already installed.\n"
	    fi
	    ;;
	*)
	    printf "\nInvalid selection.\n\n"
	    pause
	    ;;
	esac
    done
}


install_mpi()
{
    line
    # Install OpenMPI from source so we can choose build options
    if ! auto-package-installed net/openmpi; then
	if [ $USE_PORTS = 0 ]; then
	    printf "\nInstalling openmpi dependencies from packages...\n"
	    install_packages net/openmpi
	    pkg_delete /var/db/pkg/openmpi*
	fi
	printf "Reinstalling openmpi from port to customize build...\n"
	printf "Select your scheduler in the options menu.\n"
	# FIXME: make -DWITH_TORQUE or -DWITH_SGE
	install_port net openmpi
    fi
}


scheduler_config()
{
    line
    # SGE is reinstalled as a dependency of openmpi
    if auto-package-installed sysutils/sge62; then
	printf "\nUpdating /etc/services for SGE...\n"
	# Check /etc/services
	if ! fgrep -q sge_qmaster /etc/services; then
	    printf "sge_qmaster     6444/tcp   #Grid Engine Qmaster Service\n" >> /etc/services
	    printf "sge_qmaster     6444/udp   #Grid Engine Qmaster Service\n" >> /etc/services
	fi
	if ! fgrep -q sge_qmaster /etc/services; then
	    printf "sge_execd       6445/tcp   #Grid Engine Execution Service\n" >> /etc/services
	    printf "sge_execd       6445/udp   #Grid Engine Execution Service\n" >> /etc/services
	fi
	
	# Look into this later...
	# Install PVM utilities if desired
	#printf "Install SGE PVM utils? (y/n) "
	#read resp
	#if [ 0$resp = 0'y' ]; then
	#    cd $PREFIX/sge/pvm/src
	#    ./aimk
	#    ./install.sh
	#    cd $start_dir
	#fi
    fi
    
    if auto-package-installed sysutils/torque; then
	tspool="/var/spool/torque"
	if [ ! -e $tspool ]; then
	    cp -Rp $PREFIX/share/examples/torque$tspool $tspool
	fi
	
	# Package missing arrays dir
	mkdir -p $tspool/server_priv/arrays
	# Oversight in older ports.  Fixed in 2.4.6.
	# chmod 1777 /var/spool/torque/spool /var/spool/torque/undelivered
    
	# On compute nodes, put '$pbsserver ip.address.of.server' into
	# /var/spool/torque/mom_priv/config
	
	#head_ip=`fgrep $head_host /etc/hosts | awk ' { print $1 }'`
	#printf "$head_ip\n"
	
	##################################################################
	# Head node (torque server) only
	#
	
	if [ $node_type = "head" ]; then
	    # Use main interface address on server if there are multiple
	    # interfaces, not the local interface!  Torque will get
	    # confused otherwise.
	    # Also use short hostname, which must be in /etc/hosts.
	    # If someone messes up /etc/hosts, we prefer a clear lookup
	    # failure over getting the wrong address from DNS.
	    hostname -s > $tspool/server_name
    
	    # Cited as useful for dual nic servers, but not needed
	    #printf "SERVERHOST\t$head_host\n" > $tspool/torque.cfg
	    #$EDITOR $tspool/torque.cfg
    
	    # To configure queues on server, execute
	    # $PREFIX/share/examples/torque/torque.setup <admin>
	    stop_pbs

	    # Annihilates server_priv/nodes.  Edit nodes AFTER this!
	    if [ ! -e /var/spool/torque ]; then
		printf "Running torque.setup...\n"
		printf "y\n" | $PREFIX/share/examples/torque/torque.setup root > /dev/null
		sleep 5
		stop_pbs
	    fi
    
	    line
	    nodes_file=$tspool/server_priv/nodes
	    nodes_backup=$PREFIX/cluster/server_priv-nodes
	    if [ -e $nodes_backup ]; then
		cp $nodes_backup $nodes_file
	    fi
	    cat << EOM

Editing $nodes_file...

Add names of existing compute nodes and options.  Compute nodes that are
not yet operational should not be added at this time.

EOM
	    printf "Press return when ready to enter the editor..."
	    read junk
	    $EDITOR $nodes_file
	    cp $nodes_file $nodes_backup

	    # auto-enable-service doesn't start services that are already
	    # configured in rc.conf
	    auto-enable-service pbs_server cluster-setup
	    sleep 2
	    $PREFIX/etc/rc.d/pbs_server restart 2> /dev/null
	    sleep 2

	    auto-enable-service pbs_sched cluster-setup
	    sleep 2
	    $PREFIX/etc/rc.d/pbs_sched restart 2> /dev/null
	    sleep 2
	else
	    # Use local address of head node on compute nodes.  If
	    # head node has multiple interfaces, the DNS entry will
	    # provide the public interface IP rather than the private
	    # cluster IP.
	    # Also use short hostname, which must be in /etc/hosts.
	    # If someone messes up /etc/hosts, we prefer a clear lookup
	    # failure over getting the wrong address from DNS.
	    printf "$head_host\n" > $tspool/server_name
    
	    line
	    #printf "\nAdd `hostname -s` to server_priv/nodes on the server\n"
	    #printf "and restart pbs_server.\n\n"
	    #pause
	    
	    printf "Adding $(hostname -s) to server_priv/nodes on $head_host\n"
	    ssh $head_host auto-append-line "$(hostname -s)" \
		"$(hostname -s)" /var/spool/torque/server_priv/nodes \
		cluster-setup
	    ssh $head_host /usr/local/etc/rc.d/pbs_server restart
	    
	    if fgrep -q pbs_mom /etc/rc.conf; then
		if ! $PREFIX/etc/rc.d/pbs_mom stop; then
		    if killall pbs_mom 2> /dev/null; then
			printf "Old pbs_mom terminated.\n"
		    fi
		fi
	    fi

	    # On compute nodes, add the following line to /etc/rc.conf:
	    # pbs_mom_enable="YES"
	    auto-enable-service pbs_mom cluster-setup
	    sleep 5
	    $PREFIX/etc/rc.d/pbs_mom restart
	    ssh $head_host pbsnodes
	    # printf "\nTest this node by running 'pbsnodes' on head node.\n\n"
	    pause
	fi
	
	# Change torque/spool and torque/undelivered to 777?
	# Does not appear necessary anymore.
    fi
}


# Currently must come after Torque setup.  Reorg to consolidate scheduler conf
nfs_config()
{
    # Share space on head node
    line
    share="/usr"
    
    case $node_type in
    "head")
	# Configure NFS whether or not compute nodes will use it
	#resp=`auto-ask share-usr "Share $share via NFS? (y/n)" y`
	#if [ 0$resp = 0y ]; then
	    # Export /usr (which should include /usr/home)
	    exported=`awk '$1 == "'$share'" { print $1 }' /etc/exports`
	    if [ 0$exported = 0 ]; then
		# FIXME: Make sure NFS server is enabled in rc.conf first
		ip=`auto-ask local-ip "IP address of local interface?" unknown`
		# Does not work for multi-homed host
		# ip=`fgrep $head_host /etc/hosts | awk ' { print $1 }' | uniq`
		net=${ip%.[0-9]*}
		printf "Updating /etc/exports...\n"
		printf "# Generated by $0.\n" >> /etc/exports
		printf "$share\t-maproot=0\t-network $net.0 -mask 255.255.255.0\n" >> /etc/exports
		/etc/rc.d/nfsd restart
		killall -HUP mountd
		printf "\nYou may need to reboot in order to mount the new shared folder.\n\n"
		pause
	    else
		printf "$share is already configured in /etc/exports.\n"
	    fi
	#fi
	;;
	
    "compute")
    
	# Configure shared directory
	found=`awk '$1 == "'$head_host:$share'" { print $1 }' /etc/fstab`
	if [ 0$found = 0 ]; then
	    printf "\nUpdating /etc/fstab...\n\n"
	    printf "# Generated by $0.\n" >> /etc/fstab
	    printf "$head_host:$share\t\t$head_usr\tnfs\trw,intr,soft\t0\t0\n" >> /etc/fstab
	    mkdir -p $head_usr
	    mount $head_usr
	    pause
	else
	    printf "Remote filesystem $head_host:$share already configured.\n"
	fi
    
	# Add name of node to master list
	add_node_name `hostname -s` $head_usr/local/cluster/compute_nodes
	if auto-package-installed sysutils/torque; then
	    add_node_name `hostname -s` $head_usr/local/cluster/torque_nodes
	fi
	
	# Share home directories
	if [ `ls -l /home | awk ' { print $11 }'` != /head_usr/home ]; then
	    resp=`auto-ask shared-home "\nConfigure shared home directories? (y/n)" y`
	    if [ 0$resp = 0'y' ]; then
		if [ -e $head_usr/home ]; then
		    # Make sure that both "cd /home" and "cd /usr/home/..."
		    # go to $head_usr/home on compute nodes.
		    mv /usr/home /usr/home.local
		    ln -s $head_usr/home /usr
		    rm -f /home
		    ln -s $head_usr/home /
		fi
	    else
		printf "\nBe sure to create a shared data directory for users.\n\n"
		pause
	    fi
	else
	    printf 'Shared home directories already configured.\n'
	fi
	;;
	
    *)
	;;
    esac
}


# Must come after NFS setup
sge_config()
{
    case $node_type in
    "head")
	# Configure SGE master
	if auto-package-installed sysutils/sge62; then
	    #cd $PREFIX/sge
	    # -csp is for added security
	    #./install_qmaster -csp
	    #cd $start_dir
	    cell=`auto-ask sge-cell "SGE cell name [default='default']?" default`
	    if [ 0$cell = 0 ]; then
		cell='default'
	    fi
	    
	    if ! fgrep -q "sge_cell" /etc/rc.conf; then
		printf "sge_cell=\"$cell\"\n" >> /etc/rc.conf
	    fi
	    if ! fgrep -q "sge_qmaster" /etc/rc.conf; then
		printf "sge_qmaster_enable=\"YES\"\n" >> /etc/rc.conf
	    fi
	    
	    if [ ! -e $PREFIX/sge/$cell ]; then
		printf "\nRunning $PREFIX/sge/install_qmaster...\n\n"
		pause
		save_cwd=`pwd`
		cd $PREFIX/sge
		./install_qmaster
		cd $save_cwd
	    fi
	fi
	;;
    compute)
	# Configure SGE node
	if auto-package-installed sysutils/sge62; then
	    #cd $PREFIX/sge
	    #./install_execd
	    #cd $start_dir
	    
	    cell=`auto-ask sge-cell "SGE cell name [default='default']?" default`
	    if [ 0$cell = 0 ]; then
		cell='default'
	    fi
	    if ! fgrep -q "sge_cell" /etc/rc.conf; then
		printf "sge_cell=\"$cell\"\n" >> /etc/rc.conf
	    fi
	    if ! fgrep -q "sge_execd" /etc/rc.conf; then
		printf "sge_execd_enable=\"YES\"\n" >> /etc/rc.conf
	    fi
	    
	    # Link cell directory or whole SGE directory from head to
	    # $PREFIX/sge
	    ln -sf $head_usr/local/sge/$cell $PREFIX/sge
	    
	    # 
	    cat << EOM
	
Ready to run SGE setup tool...
Make sure $(hostname) is configured as an administrative host
On head node: Run qconf -sh to check and qconf -ah to add.

EOM
	    pause
    
	    #if [ ! -e $PREFIX/sge/$cell ]; then
		printf "\nRunning $PREFIX/sge/install_execd...\n\n"
		pause
		save_cwd=`pwd`
		cd $PREFIX/sge
		./install_execd
		cd $save_cwd
	    #fi
	fi
	;;
    esac
}


ganglia_config()
{
    if [ $node_type = "head" ]; then
	# Install apache
	printf "\nConfiguring apache...\n\n"
	if ! auto-package-installed www/apache22; then
	    install_packages www/apache22
	    pause
	fi

	if [ ! -e $PREFIX/etc/apache22/httpd.conf.orig ]; then
	    printf "Patching httpd.conf...\n"
	    patch -p0 < $DATADIR/patch-httpd.conf
	fi
	
	# Install a rudimentary home page
	if [ ! -e $PREFIX/www/apache22/data/global_styles.css ]; then
	    cp $DATADIR/WWW/* $PREFIX/www/apache22/data
	    ln -s /usr/local/share/doc $PREFIX/www/apache22/data/Head-node-docs
	fi
	sed -i '' -e "s|%%SHORT_HOSTNAME%%|$(hostname -s)|g" \
	    $PREFIX/www/apache22/data/index.php
	
	auto-enable-service apache22 cluster-setup
	
	# Check for php5 apache module.  It is not included in the
	# standard php5 build. Need WITH_APACHE.
	if ! auto-package-installed lang/php5 || \
	    [ ! -e $PREFIX/libexec/apache22/libphp5.so ]; then
	    # Can't use install_packages: dir name does not match port name
	    if [ ! -e $PREFIX/bin/gm4 ]; then
		install_packages devel/m4
	    fi
	    cd /usr/ports/lang/php5
	    printf "\nInstalling PHP5 for Ganglia...\n"
	    make -DBATCH -DWITH_APACHE deinstall clean reinstall
	    pause
	fi
	
	# Add date_default_timezone_set('America/Chicago'); to ganglia.php
	# or update date.timezone in $PREFIX/etc/php.ini and restart
	# Apache
	if [ ! -e $PREFIX/etc/php.ini ]; then
	    cp $PREFIX/etc/php.ini-production $PREFIX/etc/php.ini
	fi
	
	# Set timezone if not already set
	if ! grep -q '^date.timezone' $PREFIX/etc/php.ini; then
	    printf "Time zone? (Example: America/Chicago) "
	    # FIXME: Find a way to validate input
	    read zone
	    sed -i '.orig' -e "s|;date.timezone =|date.timezone = $zone|g" \
		$PREFIX/etc/php.ini
	fi
	$PREFIX/etc/rc.d/apache* restart
	pause

	# Install ganglia web frontend
	if ! auto-package-installed sysutils/ganglia-webfrontend; then
	    install_packages sysutils/ganglia-webfrontend
	fi

	if ! grep -q CLUSTER-ADMIN-PATCH-DONE $PREFIX/etc/gmetad.conf; then
	    patch -p0 < $DATADIR/patch-gmetad.conf
	fi

	# Configure ganglia
	if grep -q '%%SHORT_HOSTNAME%%' $PREFIX/etc/gmetad.conf; then
	    # patch creates gmetad.orig
	    node=1
	    nodes=''
	    while [ $node -lt $MAX_NODES ]; do
		nodes="$nodes $(printf 'compute-%03s' $node)"
		node=$(($node + 1))
	    done
	    printf "IP address of the LOCAL interface? "
	    read local_ip
	    sed -i '' -e "s|%%SHORT_HOSTNAME%%|$(hostname -s)|g" \
		      -e "s|%%LONG_HOSTNAME%%|$(hostname)|g" \
		      -e "s|%%COMPUTE_NODE_LIST%%|$nodes|g" \
		      -e "s|%%LOCAL_IP%%|$local_ip|g" \
		$PREFIX/etc/gmetad.conf
	fi
    fi
    
    line
    if ! auto-package-installed sysutils/ganglia-monitor-core; then
	install_packages sysutils/ganglia-monitor-core
	pause
    fi
    
    if grep -q 'name = "unspecified"' $PREFIX/etc/gmond.conf; then
	if [ $node_type = head ]; then
	    name=$(hostname -s)
	else
	    name=$head_host
	fi
	sed -i '.orig' \
	    -e "s|name = \"unspecified\"|name = \"$name\"|g" \
	    $PREFIX/etc/gmond.conf
    fi
    
    auto-enable-service gmond cluster-setup
    /usr/local/etc/rc.d/gmond restart
    
    if [ $node_type = "head" ]; then
	auto-enable-service gmetad cluster-setup
    fi
    
    line
    cat << EOM
    Test this node by browsing to http://$head_host/ganglia/.
    
    Note that it may take a minute or so for new nodes to appear.
EOM
    pause
}


##########################################################################
#   Function description:
#       Add this host to authorized_hosts on remote host
#
#   Arguments:
#       1) remote host
#       
#   History:
#   Date        Name        Modification
#   2013-03-01  Charlie &   Begin
##########################################################################

ssh_authorize()
{
    if [ $# -lt 1 ] ; then
       echo "usage: ssh-authorize remotehost"
       exit 1
    fi
    remotehost=$1
    
    cd
    
    user=`whoami`
    echo "Authorizing ${user}@${HOST} on $remotehost."
    ssh $remotehost 'umask 077; mkdir -p .ssh'

    # See if key already exists on remote host
    if ssh $remotehost "grep -q ${user}@${HOST} .ssh/authorized_keys"; then
	printf "Key already exists for this host.\n"
	return
    fi
    
    #
    # If a key hasn't been generated then do it.
    #
    if [ ! -f .ssh/id_dsa.pub ] ; then
	printf "Error: ssh_authorize must be called AFTER id_dsa is installed.\n"
	exit 1
    fi
    
    key=`cat .ssh/id_dsa.pub`
    ssh $remotehost "echo $key >> .ssh/authorized_keys ; chmod 600 .ssh/authorized_keys"
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

ssh_config()
{
    # Verify that nfs_config has been run
    
    line
    printf "Configuring ssh...\n"
    
    # Permit root login over ssh
    if ! grep -iq '^PermitRootLogin yes' /etc/ssh/sshd_config; then
	sed -i ".bak" 's|#PermitRootLogin no|PermitRootLogin yes|g' /etc/ssh/sshd_config
	cat << EOM

Running 'killall -HUP sshd'.  This will terminate this shell.
Log back in and re-run cluster-setup $node_type.

EOM
	pause
	killall -HUP sshd
    fi
    
    # 
    if [ $node_type = 'head' ]; then
	# Generate ssh keys
	printf "\nGenerating ssh keys for the head node...\n\n"
	id_dsa="$cluster_data/id_dsa"
	if [ ! -e $id_dsa ]; then
	    rm -rf ~/.ssh
	    ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ''
	    rm -f $id_dsa*
	    cp -p ~/.ssh/id_dsa* $cluster_data
	    chmod 600 $id_dsa*
	fi
	
	# Disable prompting for new hosts, etc.
	cluster-update-ssh_config
    else
	# Install ssh keys from head node
	if [ -e $head_usr/local/cluster/id_dsa.pub ]; then
	    mkdir -p ~/.ssh
	    chmod 0700 ~/.ssh
	    cp $head_usr/local/cluster/id_dsa* ~/.ssh
	    cp $head_usr/local/cluster/id_dsa.pub ~/.ssh/authorized_keys2
	    chmod 0600 ~/.ssh/authorized_keys2 ~/.ssh/id_dsa
	fi
	
	# Add this host to authorized keys for root on head node(s)
	ssh_authorize $head_host
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

resource_limits()
{
    # Set resource limits
    line
    if [ ! -e /etc/login.conf.orig ]; then
	cp /etc/login.conf /etc/login.conf.orig
    fi
    if [ -e $head_usr/local/cluster/login.conf ]; then
	# FIXME: replace this if with a function
	cp $head_usr/local/cluster/login.conf /etc
    else
	if grep -q maxproc=unlimited /etc/login.conf \
	    || grep -q vmemoryuse=unlimited /etc/login.conf; then
	    cat << EOM
Setting desired resource limits in /etc/login.conf.

It is a good idea to limit vmemoryuse and maxproc on the head node
to prevent it from being overloaded by careless users.

The scheduler should manage limits on compute nodes, but some generous hard
limits on vmemoryuse and maxproc in login.conf can help protect nodes from
fork() bombs and memory leaks that the scheduler does not catch (fast enough).
Usually, something like 10 + the number of cores is a good limit for compute
nodes.

EOM
	    printf "Max processes for the $node_type node? "
	    read maxproc
	    printf "Max memory use for the $node_type node? (use 'm' or 'g' suffix) "
	    read vmem
	    sed -i '' -e "s/maxproc=unlimited/maxproc=$maxproc/g" \
		     -e "s/vmemoryuse=unlimited/vmemoryuse=$vmem/g" \
		     /etc/login.conf
	    $EDITOR /etc/login.conf
	    cap_mkdb /etc/login.conf
	fi
	
	if [ $node_type = 'compute' ]; then
	    cp /etc/login.conf $head_usr/local/cluster
	fi
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

rsh_config()
{
    # Enable rsh
    if ! grep '^shell' /etc/inetd.conf || ! grep '^login' /etc/inetd.conf; then
	resp=`auto-ask enable-rshd "Enable rshd? (some MPI programs use it by default) (y/n)" y`
	if [ 0$resp = 0'y' ]; then
	    sed -i '.orig' -e 's|^#shell|shell|g' \
			   -e 's|^#login|login|g' \
		/etc/inetd.conf
	    # Add inetd to rc.conf
	    if ! fgrep inetd_enable /etc/rc.conf; then
		printf 'inetd_enable="YES"\n' >> /etc/rc.conf
	    fi
	    /etc/rc.d/inetd restart
	    # FIXME: Use auto-append-line to add to /etc/hosts.equiv
	    printf "$head_host\n" >> ~/.rhosts
	fi
    else
	printf 'rsh already configured.\n'
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-02-15  Charlie &   Begin
##########################################################################

update_system_clock()
{
    line
    printf "Updating system clock...\n"
    if /etc/rc.d/ntpd stop 2 /dev/null 2>&1; then
	printf "Stopped NTP.\n"
    fi
    ntpdate pool.ntp.org
    auto-enable-service ntpd cluster-setup > /dev/null 2>&1
    /etc/rc.d/ntpd restart
}


##########################################################################
#   Main
##########################################################################

if [ $# != 1 ]; then
    usage $0
fi

if [ `whoami` != root ]; then
    printf "$0 must be run as root.\n"
    exit 1
fi

node_type=$1
case $node_type in
"head"|"compute")
    ;;
*)
    printf "Invalid installation type: $node_type\n"
    usage $0
esac

update_system_clock

line
auto-enable-procfs

line
resp=`auto-ask use-source "Build/install ports from source? (y/n)" n`
if [ 0$resp = 0'y' ]; then
    USE_PORTS=1
else
    USE_PORTS=0
    if [ x$AUTO_PACKAGEROOT = x ]; then
	printf "Finding fastest mirror... "
	export AUTO_PACKAGEROOT=`auto-fastest-mirror`
	printf "$AUTO_PACKAGEROOT\n"
    fi
fi

if [ 0$EDITOR = 0 ]; then
    EDITOR=vi
    export EDITOR
fi

# Quick-install some common tools
line
printf "Installing common tools...\n"
install_packages devel/gmake net/rsync converters/libiconv security/sudo

head_usr="/head_usr"
start_dir=`pwd`
PREFIX="/usr/local"
DATADIR=$PREFIX/share/cluster-admin
# No more than 245 if netmask is ffffff00.  Compute node addresses start at 11.
MAX_NODES=100
cluster_data="$PREFIX/cluster"
mkdir -p $cluster_data

edit_hosts
install_scheduler
# MPI is not used on the head node, but needed for scheduler config?
# Maybe this should only be installed on compute nodes.
if [ $node_type = 'compute' ]; then
    install_mpi
fi
nfs_config
ssh_config          # After nfs_config
scheduler_config    # After ssh_config
resource_limits     # After nfs_config

if auto-package-installed sysutils/sge62; then
    sge_config
fi
ganglia_config

line
case $node_type in
"head")
    
    # Install extra login logic
    line
    printf "Installing startup scripts...\n"
    if [ ! -e $PREFIX/cluster/profile ]; then
	cp $DATADIR/profile $PREFIX/cluster
    fi
    if [ ! -e $PREFIX/cluster/csh.login ]; then
	cp $DATADIR/csh.login $PREFIX/cluster
    fi
    auto-append-line cluster/profile ". $PREFIX/cluster/profile" /etc/profile cluster-setup
    auto-append-line cluster/csh.login "source $PREFIX/cluster/csh.login" /etc/csh.login cluster-setup
    cluster-update-all-ssh_config
    ;;

"compute")

    rsh_config
    ssh $head_host cluster-update-all-ssh_config
    pause

    ;;
*)
    ;;
esac

line
cat << EOM

You must reboot to test the new configuration.  If this is the first time
you completed the cluster-setup $node_type process, you should reboot now.

EOM

printf "Reboot? (y/[n]) "
read reboot
if [ 0$reboot = 0y ]; then
    shutdown -r now
fi

