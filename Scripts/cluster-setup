#!/bin/sh -e

##########################################################################
#   Description:
#       Automate setup of a simple FreeBSD cluster.
#       Installs common software and configures settings to facilitate
#       cluster operation.  Assumes a single head node will handle
#       job initiation/scheduling and act as a file server for all nodes.
#
#   Usage:
#       First, run 
#
#           cluster-setup head
#
#       on the head node.
#
#   Then, run
#
#           cluster-setup compute
#
#       on the rest.
#       
#   History:
#       Dec 2009    J Bacon
##########################################################################


usage()
{
    printf "Usage: $0 head|compute\n"
    exit 1
}


line()
{
    printf "==============================================================================\n"
}


pause()
{
    local junk
    printf "Press return to continue..."
    read junk
}


# FIXME: Base this on stop_service
stop_pbs()
{
    printf "Stopping PBS daemons...\n"
    if fgrep -q pbs_sched_enable $RC_CONF; then
	service pbs_sched stop > /dev/null 2>&1 || true
	sleep 2
    fi
    if fgrep -q pbs_server_enable $RC_CONF; then
	service pbs_server stop > /dev/null 2>&1 || true
	sleep 2
    fi
    if killall pbs_sched > /dev/null 2>&1; then
	printf "Rogue pbs_sched terminated.\n"
	sleep 2
    fi
    if killall pbs_server > /dev/null 2>&1; then
	printf "Rogue pbs_server terminated.\n"
	sleep 2
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-03  Charlie &   Begin
##########################################################################

stop_service()
{
    if [ $# != 1 ]; then
	printf "Usage: stop_service service\n"
    fi
    service $1 stop > /dev/null 2>&1 || true
    killall $1 > /dev/null 2>&1 || true
}


##########################################################################
#   Description:
#       Install ports or packages specified by arguments if they are not
#       already installed.  If $AUTO_BUILD_FROM_SOURCE is true, installs from source,
#       otherwise attempts to install using pkg_add.
#
#   Arguments:
#       List of ports in the for "category/name"
##########################################################################

install_packages()
{
    printf "Installing:\n"
    for pkg in $*; do
    {
	printf "\t$pkg...\n"
	if [ ! -e $LOG_DIR ]; then
	    mkdir $LOG_DIR
	fi
	log_file=$LOG_DIR/`echo $pkg | tr / -`
	if ! auto-install-packages $pkg > $log_file 2>&1; then
	    more $log_file
	    exit 1
	fi
    }
    done
}


install_port()
{
    local resp category port
    
    if [ $# -lt 2 ]; then
	printf "Usage: install_port category port [make flags]\n"
	exit 1
    fi
    
    category=$1
    port=$2
    shift
    shift
    # FIXME: Use cluster-pkgdir
    if [ -e $HEAD_USR/ports/packages/All ] && \
	ls $HEAD_USR/ports/packages/All/${port}* ; then
	pkg_add $HEAD_USR/ports/packages/All/${port}*
    else
	cd /usr/ports/$category/$port
	make deinstall
	make clean
	make rmconfig
	make reinstall
    fi
}


add_node_name()
{
    if [ $# != 2 ]; then
	printf "Usage: $0 node-name node-file\n"
	exit 1
    fi
    
    name=$1
    file=$2
    
    # Prevent fgrep from failing
    if [ ! -e $file ]; then
	touch $file
    fi
    
    if ! fgrep -q $name $file; then
	printf "$name\n" >> $file
    fi
}


edit_hosts()
{
    line
    printf "Updating /etc/hosts file...\n"
    if [ $NODE_TYPE = "compute" ]; then
	# If cluster-setup was previously completed on this node,
	# get head node from configuration, otherwise ask.
	if [ -e $HEAD_USR/local/cluster/head_node ]; then
	    head_host=`cat $HEAD_USR/local/cluster/head_node`
	else
	    head_host=''
	    while [ 0$head_host = 0 ]; do
		head_host=`auto-ask head-hostname "Short hostname of head node?" unknown`
	    done
	fi
    else
	head_host=`hostname -s`
	hostname -s > $LOCALBASE/cluster/head_node
    fi

    # Add head node to local /etc/hosts
    head_host_entry="$HEAD_IP\t\t$head_host.local $head_host\n"
    auto-append-line 'cluster-setup' \
	"\n# Entries generated by cluster-setup\n" /etc/hosts nocomment
    auto-append-line "$head_host_entry" "$head_host_entry" /etc/hosts nocomment
    
    # Add compute node to both local and head node /etc/hosts
    if [ $NODE_TYPE = "compute" ]; then
	compute_ip=`auto-ask compute-ip "Local IP address of this compute node?" unknown`
	short_host=`hostname -s`
	compute_host_entry="$compute_ip\t\t$short_host.local $short_host\n"
	if ! ssh $head_host auto-append-line "\"$compute_host_entry\"" \
		"\"$compute_host_entry\"" /etc/hosts nocomment; then
	    cat << EOM

Unable to automatically update ${head_host}:/etc/hosts.

Add the following line to /etc/hosts on the head node:

EOM
	    printf "$compute_host_entry"
	    pause
	fi
    fi
}


install_scheduler()
{
    line
    # Patch bug in openmpi run_depends
    sed -i '.bak' 's|${SGE_ROOT}/fbsd-${ARCH}/qrsh|${SGE_ROOT}/bin/fbsd-${ARCH}/qrsh|g' \
	/usr/ports/net/openmpi/Makefile
    
    cat << EOM
Please choose a scheduler:

1.. HTCondor
2.. SGE (Requires X11)
3.. SLURM
4.. Torque PBS

EOM

    resp=' '
    while [ 0$resp != 01 ] && [ 0$resp != 02 ] && [ 0$resp != 03 ] && [ 0$resp != 04 ]; do
	resp=`auto-ask scheduler "Scheduler selection?" unknown`
	case $resp in
	1)
	    install_packages sysutils/condor
	    pause
	    ;;
	2)
	    if ! auto-package-installed sysutils/sge62; then
		install_packages sysutils/sge62
	    
		line
		printf "You can ignore the post-install messages above.\n"
		printf "The services will be configured automatically.\n"
		pause
		
		# Source $LOCALBASE/sge/default/common/settings.csh
		# All execution hosts must be administrative hosts during the installation.
		# All hosts which you added to the list of administrative hosts during this
		# installation procedure can now be installed.
		line
		printf "\nAll users should source $LOCALBASE/sge/<cell>/common/settings.[c]sh\n\n"
		pause
	    else
		printf "SGE is already installed.\n"
	    fi
	    ;;
	3)
	    # Install dependencies
	    install_packages sysutils/slurm-hpc
	    
	    # Reinstall with custom options
	    reinstall=`auto-ask reinstall-slurm "Rebuild slurm with custom options?" n`
	    if [ $reinstall = 'y' ]; then
		install_port sysutils slurm-hpc
	    fi
	    ;;
	4)
	    if ! auto-package-installed sysutils/torque; then
		line
		install_packages sysutils/torque
		# Package does not set permissions on spool directories
		# Also, users are encouraged to update sysutils/torque
		# to the latest version instead of using the package
		#pkg_delete /var/db/pkg/torque*
		#install_packages lang/perl devel/libtool
		#printf "Reinstalling torque from port to ...\n"
		#install_port sysutils torque
	    
		line
		printf "You can ignore the post-install messages above.\n"
		printf "The services will be configured automatically.\n"
		pause
	    fi
	    ;;
	*)
	    printf "\nInvalid selection.\n\n"
	    pause
	    ;;
	esac
    done
}


install_mpi()
{
    line
    # Install OpenMPI from source so we can choose build options
    if ! auto-package-installed net/openmpi; then
	if [ $AUTO_BUILD_FROM_SOURCE = 0 ]; then
	    printf "\nInstalling openmpi dependencies from packages...\n"
	    install_packages net/openmpi
	    pkg_delete /var/db/pkg/openmpi*
	fi
	printf "Reinstalling openmpi from port to customize build...\n"
	printf "Select your scheduler in the options menu.\n"
	# FIXME: make -DWITH_TORQUE or -DWITH_SGE
	install_port net openmpi
    fi
}


scheduler_config()
{
    line
    if auto-package-installed sysutils/slurm-hpc; then
	cluster-slurm-setup $NODE_TYPE
    fi
    
    if auto-package-installed sysutils/condor; then
	if [ ! -e $LOCALBASE/etc/condor_config ]; then
	    cp $LOCALBASE/etc/condor/condor_config $LOCALBASE/etc
	fi
	if [ ! -e $LOCALBASE/etc/condor_config.local ]; then
	    cp $LOCALBASE/etc/condor/condor_config.local $LOCALBASE/etc
	fi
	$EDITOR /usr/local/etc/condor_config*
    fi
    
    # SGE is reinstalled as a dependency of openmpi
    if auto-package-installed sysutils/sge62; then
	sge_config      # Run SGE setup tools
	
	printf "\nUpdating /etc/services for SGE...\n"
	# Check /etc/services
	if ! fgrep -q sge_qmaster /etc/services; then
	    printf "sge_qmaster     6444/tcp   #Grid Engine Qmaster Service\n" >> /etc/services
	    printf "sge_qmaster     6444/udp   #Grid Engine Qmaster Service\n" >> /etc/services
	fi
	if ! fgrep -q sge_qmaster /etc/services; then
	    printf "sge_execd       6445/tcp   #Grid Engine Execution Service\n" >> /etc/services
	    printf "sge_execd       6445/udp   #Grid Engine Execution Service\n" >> /etc/services
	fi
	
	# Look into this later...
	# Install PVM utilities if desired
	#printf "Install SGE PVM utils? (y/n) "
	#read resp
	#if [ 0$resp = 0'y' ]; then
	#    cd $LOCALBASE/sge/pvm/src
	#    ./aimk
	#    ./install.sh
	#    cd $start_dir
	#fi
    fi
    
    if auto-package-installed sysutils/torque; then
	if [ ! -e $TSPOOL ]; then
	    cp -Rp $LOCALBASE/share/examples/torque$TSPOOL $TSPOOL
	    chmod 1777 $TSPOOL/checkpoint $TSPOOL/spool $TSPOOL/undelivered
	fi
	
	# Package missing arrays dir
	mkdir -p $TSPOOL/server_priv/arrays
	
	# On compute nodes, put '$pbsserver ip.address.of.server' into
	# $TSPOOL/mom_priv/config
	
	##################################################################
	# Head node (torque server) only
	#
	
	if [ $NODE_TYPE = "head" ]; then
	    # Use main interface address on server if there are multiple
	    # interfaces, not the local interface!  Torque will get
	    # confused otherwise.
	    # Also use short hostname, which must be in /etc/hosts.
	    # If someone messes up /etc/hosts, we prefer a clear lookup
	    # failure over getting the wrong address from DNS.
	    hostname -s > $TSPOOL/server_name
    
	    # Cited as useful for dual nic servers, but not needed
	    #printf "SERVERHOST\t$head_host\n" > $TSPOOL/torque.cfg
	    #$EDITOR $TSPOOL/torque.cfg
    
	    # To configure queues on server, execute
	    # $LOCALBASE/share/examples/torque/torque.setup <admin>
	    stop_pbs

	    # Annihilates server_priv/nodes.  Edit nodes AFTER this!
	    #if [ ! -e $TSPOOL ]; then
		printf "Running torque.setup...\n"
		printf "y\n" | $LOCALBASE/share/examples/torque/torque.setup root \
		    > $LOG_DIR/torque.setup
		sleep 5
		stop_pbs
	    #fi
    
	    line
	    nodes_file=$TSPOOL/server_priv/nodes
	    nodes_backup=$LOCALBASE/cluster/server_priv-nodes
	    if [ -e $nodes_backup ]; then
		cp $nodes_backup $nodes_file
	    fi
	    cat << EOM

Editing $nodes_file...

Add names of existing compute nodes and options.  Compute nodes that are
not yet operational should not be added at this time.

EOM
	    printf "Press return when ready to enter the editor..."
	    read junk
	    $EDITOR $nodes_file
	    cp $nodes_file $nodes_backup

	    # auto-enable-service doesn't start services that are already
	    # configured in rc.conf
	    auto-enable-service pbs_server cluster-setup \
		> $LOG_DIR/pbs_server_enable 2>&1
	    sleep 2
	    service pbs_server restart 2> $LOG_DIR/pbs_server_start
	    sleep 2

	    auto-enable-service pbs_sched cluster-setup 
		> $LOG_DIR/pbs_sched_enable 2>&1
	    sleep 2
	    service pbs_sched restart 2> $LOG_DIR/pbs_sched_start
	    sleep 2
	else
	    # Use local address of head node on compute nodes.  If
	    # head node has multiple interfaces, the DNS entry will
	    # provide the public interface IP rather than the private
	    # cluster IP.
	    # Also use short hostname, which must be in /etc/hosts.
	    # If someone messes up /etc/hosts, we prefer a clear lookup
	    # failure over getting the wrong address from DNS.
	    printf "$head_host\n" > $TSPOOL/server_name
    
	    line
	    #printf "\nAdd `hostname -s` to server_priv/nodes on the server\n"
	    #printf "and restart pbs_server.\n\n"
	    #pause
	    
	    printf "Adding $(hostname -s) to server_priv/nodes on $head_host\n"
	    ssh $head_host auto-append-line "$(hostname -s)" \
		"$(hostname -s)" $TSPOOL/server_priv/nodes \
		cluster-setup
	    ssh $head_host service pbs_server restart
	    
	    if fgrep -q pbs_mom $RC_CONF; then
		if ! service pbs_mom stop > /dev/null 2>&1; then
		    if killall pbs_mom > /dev/null 2>&1; then
			printf "Old pbs_mom terminated.\n"
		    fi
		fi
	    fi

	    # On compute nodes, add the following line to $RC_CONF:
	    # pbs_mom_enable="YES"
	    auto-enable-service pbs_mom cluster-setup \
		> $LOG_DIR/pbs_mom_enable 2>&1
	    sleep 5
	    service pbs_mom restart
	    ssh $head_host pbsnodes
	    # printf "\nTest this node by running 'pbsnodes' on head node.\n\n"
	    pause
	fi
	
	# Change torque/spool and torque/undelivered to 777?
	# Does not appear necessary anymore.
    fi
}


# Currently must come after Torque setup.  Reorg to consolidate scheduler conf
nfs_config()
{
    # Share space on head node
    line
    share="/usr"
    
    case $NODE_TYPE in
    "head")
	# Configure NFS whether or not compute nodes will use it
	# Export /usr (which should include /usr/home)
	exported=`awk '$1 == "'$share'" { print $1 }' /etc/exports`
	if [ 0$exported = 0 ]; then
	    # FIXME: Make sure NFS server is enabled in rc.conf first
	    # Does not work for multi-homed host
	    # ip=`fgrep $head_host /etc/hosts | awk ' { print $1 }' | uniq`
	    net=${$HEAD_IP%.[0-9]*}

	    printf "Updating /etc/exports...\n"
	    auto-append-line "$share\t-maproot=0\t-network $net.0 -mask 255.255.255.0\n" /etc/exports $0
	    
	    printf "Updating $RC_CONF...\n"
	    auto-append-line 'nfs_server_flags="-t -n 16"' $RC_CONF $0
	    service nfsd restart
	    killall -HUP mountd
	    printf "\nYou may need to reboot before NFS clients can mount the new shared folder.\n\n"
	    pause
	else
	    printf "$share is already configured in /etc/exports.\n"
	fi
	;;
	
    "compute")
    
	# Configure shared directory
	found=`awk '$1 == "'$head_host:$share'" { print $1 }' /etc/fstab`
	if [ 0$found = 0 ]; then
	    printf "\nUpdating /etc/fstab...\n\n"
	    printf "# Generated by $0.\n" >> /etc/fstab
	    printf "# Adjust rsize and wsize if needed to improve performance.\n" >> /etc/fstab
	    printf "$head_host:$share\t\t$HEAD_USR\tnfs\trw,intr,noatime,rsize=8192,wsize=8192\t0\t0\n" >> /etc/fstab
	else
	    printf "Remote filesystem $head_host:$share already configured.\n"
	fi
	mkdir -p $HEAD_USR
	mount $HEAD_USR
	pause
    
	# Add name of node to master list
	add_node_name `hostname -s` $HEAD_USR/local/cluster/compute_nodes
	if auto-package-installed sysutils/torque; then
	    add_node_name `hostname -s` $HEAD_USR/local/cluster/torque_nodes
	fi
	
	# Share home directories
	if [ `ls -l /home | awk ' { print $11 }'` != /HEAD_USR/home ]; then
	    resp=`auto-ask shared-home "\nConfigure shared home directories? (y/n)" y`
	    if [ 0$resp = 0'y' ]; then
		if [ -e $HEAD_USR/home ]; then
		    # Make sure that both "cd /home" and "cd /usr/home/..."
		    # go to $HEAD_USR/home on compute nodes.
		    mv /usr/home /usr/home.local
		    ln -s $HEAD_USR/home /usr
		    rm -f /home
		    ln -s $HEAD_USR/home /
		fi
	    fi
	else
	    printf 'Shared home directories already configured.\n'
	fi
	;;
	
    *)
	;;
    esac
}


# Must come after NFS setup
sge_config()
{
    case $NODE_TYPE in
    "head")
	# Configure SGE master
	if auto-package-installed sysutils/sge62; then
	    #cd $LOCALBASE/sge
	    # -csp is for added security
	    #./install_qmaster -csp
	    #cd $start_dir
	    cell=`auto-ask sge-cell "SGE cell name [default='default']?" default`
	    if [ 0$cell = 0 ]; then
		cell='default'
	    fi
	    
	    if ! fgrep -q "sge_cell" $RC_CONF; then
		printf "sge_cell=\"$cell\"\n" >> $RC_CONF
	    fi
	    if ! fgrep -q "sge_qmaster" $RC_CONF; then
		printf "sge_qmaster_enable=\"YES\"\n" >> $RC_CONF
	    fi
	    
	    if [ ! -e $LOCALBASE/sge/$cell ]; then
		printf "\nRunning $LOCALBASE/sge/install_qmaster...\n\n"
		pause
		save_cwd=`pwd`
		cd $LOCALBASE/sge
		./install_qmaster
		cd $save_cwd
	    fi
	fi
	;;
    compute)
	# Configure SGE node
	if auto-package-installed sysutils/sge62; then
	    #cd $LOCALBASE/sge
	    #./install_execd
	    #cd $start_dir
	    
	    cell=`auto-ask sge-cell "SGE cell name [default='default']?" default`
	    if [ 0$cell = 0 ]; then
		cell='default'
	    fi
	    if ! fgrep -q "sge_cell" $RC_CONF; then
		printf "sge_cell=\"$cell\"\n" >> $RC_CONF
	    fi
	    if ! fgrep -q "sge_execd" $RC_CONF; then
		printf "sge_execd_enable=\"YES\"\n" >> $RC_CONF
	    fi
	    
	    # Link cell directory or whole SGE directory from head to
	    # $LOCALBASE/sge
	    ln -sf $HEAD_USR/local/sge/$cell $LOCALBASE/sge
	    
	    # 
	    cat << EOM
	
Ready to run SGE setup tool...
Make sure $(hostname) is configured as an administrative host
On head node: Run qconf -sh to check and qconf -ah to add.

EOM
	    pause
    
	    #if [ ! -e $LOCALBASE/sge/$cell ]; then
		printf "\nRunning $LOCALBASE/sge/install_execd...\n\n"
		pause
		save_cwd=`pwd`
		cd $LOCALBASE/sge
		./install_execd
		cd $save_cwd
	    #fi
	fi
	;;
    esac
}


ganglia_config()
{
    if [ $NODE_TYPE = "head" ]; then
	# Install apache
	line
	printf "Configuring apache...\n"
	if ! auto-package-installed www/apache22; then
	    install_packages www/apache22
	    pause
	fi

	if [ ! -e $LOCALBASE/etc/apache22/httpd.conf.orig ]; then
	    printf "Patching httpd.conf...\n"
	    patch -p0 < $DATADIR/patch-httpd.conf
	fi
	
	# Install a rudimentary home page
	if [ ! -e $LOCALBASE/www/apache22/data/global_styles.css ]; then
	    cp $DATADIR/WWW/* $LOCALBASE/www/apache22/data
	    ln -s /usr/local/share/doc $LOCALBASE/www/apache22/data/Head-node-docs
	fi
	sed -i '' -e "s|%%SHORT_HOSTNAME%%|$(hostname -s)|g" \
	    $LOCALBASE/www/apache22/data/index.php
	
	# Check for php5 apache module.  It is not included in the
	# standard php5 build. Need WITH_APACHE.
	if ! auto-package-installed lang/php5 || \
	    [ ! -e $LOCALBASE/libexec/apache22/libphp5.so ]; then
	    install_packages lang/php5
	    #if [ ! -e $LOCALBASE/bin/gm4 ]; then
	    #    install_packages devel/m4
	    #fi
	    cd /usr/ports/lang/php5
	    printf "\nInstalling PHP5 for Ganglia...\n"
	    make -DBATCH -DWITH_APACHE deinstall clean reinstall
	    pause
	fi

	# Must come after php5 install, since http.conf was patched
	auto-enable-service apache22 cluster-setup \
	    > $LOG_DIR/apache22_enable 2>&1
	
	# Add date_default_timezone_set('America/Chicago'); to ganglia.php
	# or update date.timezone in $LOCALBASE/etc/php.ini and restart
	# Apache
	if [ ! -e $LOCALBASE/etc/php.ini ]; then
	    cp $LOCALBASE/etc/php.ini-production $LOCALBASE/etc/php.ini
	fi
	
	# Set timezone if not already set
	if ! grep -q '^date.timezone' $LOCALBASE/etc/php.ini; then
	    printf "Time zone? (Example: America/Chicago) "
	    # FIXME: Find a way to validate input
	    read zone
	    sed -i '.orig' -e "s|;date.timezone =|date.timezone = $zone|g" \
		$LOCALBASE/etc/php.ini
	fi
	
	line
	# Install ganglia web frontend
	if ! auto-package-installed sysutils/ganglia-webfrontend; then
	    install_packages sysutils/ganglia-webfrontend
	fi

	# patch creates gmetad.orig
	if ! grep -q CLUSTER-ADMIN-PATCH-DONE $LOCALBASE/etc/gmetad.conf; then
	    patch -p0 < $DATADIR/patch-gmetad.conf
	fi

	# Configure ganglia
	if grep -q '%%SHORT_HOSTNAME%%' $LOCALBASE/etc/gmetad.conf; then
	    # FIXME: gmetad crashes if COMPUTE_NODE_LIST is too long
	    sed -i '' -e "s|%%SHORT_HOSTNAME%%|$(hostname -s)|g" \
		      -e "s|%%LONG_HOSTNAME%%|$(hostname)|g" \
		      -e "s|%%LOCAL_IP%%|$HEAD_IP|g" \
		$LOCALBASE/etc/gmetad.conf
	fi

	line
	stop_service apache22
	service apache22 start
    else
	line
	cat << EOM
If your head node is serving as a gateway, you may need to add
$(hostname -s) to the data_source line in 
${head_host}:/usr/local/etc/gmetad.conf.
EOM
	pause
    fi
    
    line
    if ! auto-package-installed sysutils/ganglia-monitor-core; then
	install_packages sysutils/ganglia-monitor-core
    fi
    
    if grep -q 'name = "unspecified"' $LOCALBASE/etc/gmond.conf; then
	if [ $NODE_TYPE = head ]; then
	    name=$(hostname -s)
	else
	    name=$head_host
	fi
	sed -i '.orig' \
	    -e "s|name = \"unspecified\"|name = \"$name\"|g" \
	    $LOCALBASE/etc/gmond.conf
    fi
    
    
    stop_service gmond
    auto-enable-service gmond cluster-setup \
	> $LOG_DIR/gmond_enable 2>&1
    service gmond start
    
    if [ $NODE_TYPE = "head" ]; then
	stop_service gmetad
	auto-enable-service gmetad cluster-setup \
	    > $LOG_DIR/gmetad_enable 2>&1
	service gmetad start
    fi
    
    line
    cat << EOM
    Test this node by browsing to http://$head_host/ganglia/.
    
    Note that it may take a minute or so for new nodes to appear.
EOM
    pause
}


##########################################################################
#   Function description:
#       Add this host to authorized_hosts on remote host
#
#   Arguments:
#       1) remote host
#       
#   History:
#   Date        Name        Modification
#   2013-03-01  Charlie &   Begin
##########################################################################

ssh_authorize()
{
    if [ $# -lt 1 ] ; then
       echo "usage: ssh-authorize remotehost"
       exit 1
    fi
    remotehost=$1
    
    cd
    
    user=`whoami`
    echo "Authorizing ${user}@${HOST} on $remotehost."
    ssh $remotehost 'umask 077; mkdir -p .ssh'

    # See if key already exists on remote host
    if ssh $remotehost "grep -q ${user}@${HOST} .ssh/authorized_keys"; then
	printf "Key already exists for this host.\n"
	return
    fi
    
    #
    # If a key hasn't been generated then do it.
    #
    if [ ! -f .ssh/id_dsa.pub ] ; then
	printf "Error: ssh_authorize must be called AFTER id_dsa is installed.\n"
	exit 1
    fi
    
    key=`cat .ssh/id_dsa.pub`
    ssh $remotehost "echo $key >> .ssh/authorized_keys ; chmod 600 .ssh/authorized_keys"
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

ssh_config()
{
    # Verify that nfs_config has been run

    line
    printf "Configuring ssh...\n"
    
    # Permit root login over ssh
    if ! grep -iq '^PermitRootLogin yes' /etc/ssh/sshd_config; then
	sed -i ".bak" 's|#PermitRootLogin no|PermitRootLogin yes|g' /etc/ssh/sshd_config
	line
	cat << EOM
WARNING: PermitRootLogin has been enabled.  This is necessary for clusters
that use keyed SSH between nodes for system activities.  If you are concerned
about security, you can manually reconfigure the head node.
EOM
	cat << EOM

Running 'killall -HUP sshd'.  
If you are logged in via ssh, this will terminate your connection.
Log back in and re-run cluster-setup $NODE_TYPE.

EOM
	pause
	killall -HUP sshd
    fi
    
    # 
    if [ $NODE_TYPE = 'head' ]; then
	# Generate ssh keys
	printf "Generating ssh keys for the head node...\n"
	id_dsa="$CLUSTER_DATA/id_dsa"
	if [ ! -e $id_dsa ]; then
	    rm -rf ~/.ssh
	    ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ''
	    rm -f $id_dsa*
	    cp -p ~/.ssh/id_dsa* $CLUSTER_DATA
	    chmod 600 $id_dsa*
	fi
	
	# Disable prompting for new hosts, etc.
	cluster-update-ssh_config
    else
	# Install ssh keys from head node
	if [ -e $HEAD_USR/local/cluster/id_dsa.pub ]; then
	    mkdir -p ~/.ssh
	    chmod 0700 ~/.ssh
	    cp $HEAD_USR/local/cluster/id_dsa* ~/.ssh
	    cp $HEAD_USR/local/cluster/id_dsa.pub ~/.ssh/authorized_keys2
	    chmod 0600 ~/.ssh/authorized_keys2 ~/.ssh/id_dsa
	fi
	
	# Add this host to authorized keys for root on head node(s)
	ssh_authorize $head_host
    fi
}


##########################################################################
#   Function description:
#       Must be done after NFS config due to use of $HEAD_USR.
#
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

resource_limits()
{
    # Set resource limits
    line
    printf "Configuring resource limits...\n"
    if [ ! -e /etc/login.conf.orig ]; then
	cp /etc/login.conf /etc/login.conf.orig
    fi
    if [ -e $HEAD_USR/local/cluster/login.conf ]; then
	# FIXME: replace this if with a function
	cp $HEAD_USR/local/cluster/login.conf /etc
    else
	if grep -q maxproc=unlimited /etc/login.conf \
	    || grep -q vmemoryuse=unlimited /etc/login.conf; then
	    cat << EOM
Setting desired resource limits in /etc/login.conf.

It is a good idea to limit vmemoryuse and maxproc on the head node
to prevent it from being overloaded by careless users.

The scheduler should manage limits on compute nodes, but some generous hard
limits on vmemoryuse and maxproc in login.conf can help protect nodes from
fork() bombs and memory leaks that the scheduler does not catch (fast enough).
Usually, something like 10 + the number of cores is a good limit for compute
nodes.

EOM
	    printf "Max processes for the $NODE_TYPE node? "
	    read maxproc
	    printf "Max memory use for the $NODE_TYPE node? (use 'm' or 'g' suffix) "
	    read vmem
	    sed -i '' -e "s/maxproc=unlimited/maxproc=$maxproc/g" \
		     -e "s/vmemoryuse=unlimited/vmemoryuse=$vmem/g" \
		     /etc/login.conf
	    $EDITOR /etc/login.conf
	    cap_mkdb /etc/login.conf
	fi
	
	if [ $NODE_TYPE = 'compute' ]; then
	    cp /etc/login.conf $HEAD_USR/local/cluster
	fi
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

rsh_config()
{
    # Enable rsh
    if ! grep '^shell' /etc/inetd.conf || ! grep '^login' /etc/inetd.conf; then
	resp=`auto-ask enable-rshd "Enable rshd? (some MPI programs use it by default) (y/n)" y`
	if [ 0$resp = 0'y' ]; then
	    sed -i '.orig' -e 's|^#shell|shell|g' \
			   -e 's|^#login|login|g' \
		/etc/inetd.conf
	    # Add inetd to rc.conf
	    if ! fgrep inetd_enable $RC_CONF; then
		printf 'inetd_enable="YES"\n' >> $RC_CONF
	    fi
	    service inetd restart
	    # FIXME: Use auto-append-line to add to /etc/hosts.equiv
	    printf "$head_host\n" >> ~/.rhosts
	fi
    else
	printf 'rsh already configured.\n'
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-02-15  Charlie &   Begin
##########################################################################

update_system_clock()
{
    line
    printf "Updating system clock...\n"
    stop_service ntpd
    if killall -9 ntpdate > /dev/null 2>&1; then
	printf "Killed rogue ntpdate.\n"
    fi
    ntpdate pool.ntp.org
    auto-enable-service ntpd cluster-setup > /dev/null 2>&1
    service ntpd start
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

install_common_tools()
{
    # Quick-install some common tools
    line
    install_packages devel/gmake net/rsync converters/libiconv security/sudo
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

get_port_install_method()
{
    line
    resp=`auto-ask use-source "Build ports from source?" n`
    if [ 0$resp = 0'y' ]; then
	export AUTO_BUILD_FROM_SOURCE='yes'
    else
	export AUTO_BUILD_FROM_SOURCE='fall-back'
	if [ x$AUTO_PACKAGEROOT = x ]; then
	    printf "Finding fastest mirror... "
	    export AUTO_PACKAGEROOT=`auto-fastest-mirror`
	    printf "$AUTO_PACKAGEROOT\n"
	fi
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

startup_scripts()
{
    # Install extra login logic
    line
    printf "Installing startup scripts...\n"
    if [ ! -e $LOCALBASE/cluster/profile ]; then
	cp $DATADIR/profile $LOCALBASE/cluster
    fi
    if [ ! -e $LOCALBASE/cluster/csh.login ]; then
	cp $DATADIR/csh.login $LOCALBASE/cluster
    fi
    auto-append-line cluster/profile ". $LOCALBASE/cluster/profile" /etc/profile cluster-setup
    auto-append-line cluster/csh.login "source $LOCALBASE/cluster/csh.login" /etc/csh.login cluster-setup
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

enable_procfs()
{
    line
    auto-enable-procfs
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2013-12-02  Jason Bacon Begin
##########################################################################

reboot_maybe()
{
    line
    cat << EOM

You must reboot to test the new configuration.  If this is the first time
you completed the cluster-setup $NODE_TYPE process, you should reboot now.

EOM

    printf "Reboot? (y/[n]) "
    read reboot
    if [ 0$reboot = 0y ]; then
	shutdown -r now
    fi
}


##########################################################################
#   Main
##########################################################################

if [ $# != 1 ]; then
    usage $0
fi

if [ `whoami` != root ]; then
    printf "$0 must be run as root.\n"
    exit 1
fi


##########################################################################
#   Set common variables
##########################################################################

if [ 0$EDITOR = 0 ]; then
    EDITOR=vi
    export EDITOR
fi

# Recommended layout for dedicated hardware:
# Use netmask of 255.255.0.0.
# Head node is 10.1.1.1 if it's a gateway or 10.1.1.2 with a separate router.
# 10.1.1.3 - 10.1.1.254 are for extra head nodes, I/O nodes, etc.
# Compute nodes begin at 10.1.2.1
MAX_NODES=60000

# Mount point for head node /usr
HEAD_USR="/head_usr"

# Misc
LOCALBASE="/usr/local"
RC_CONF='/etc/rc.conf'
START_DIR=`pwd`

# Used by child scripts, so export it
export LOG_DIR=$START_DIR/desktop-installer-log
line
printf "Command output will be stored in $LOG_DIR.\n"
line
pause

# Config files used by cluster-admin
CLUSTER_DATA="$LOCALBASE/cluster"
mkdir -p $CLUSTER_DATA

# Source for default files
DATADIR=$LOCALBASE/share/cluster-admin

TSPOOL="/var/spool/torque"

NODE_TYPE=$1
case $NODE_TYPE in
"head"|"compute")
    ;;
*)
    printf "Invalid installation type: $NODE_TYPE\n"
    usage $0
esac

HEAD_IP=`auto-ask head-ip 'IP address of head node LOCAL interface?' unknown`

##########################################################################
#   Begin setup
##########################################################################

# The following configuration must be done on all nodes, although
# different node types may be configured differently.
update_system_clock         # Start with and NTP update
enable_procfs               # Needed for?
get_port_install_method     # Build ports from source?
install_common_tools        # gmake, etc.
edit_hosts                  # Add new nodes to hosts files
install_scheduler           # Select and install a scheduler
nfs_config                  # Set up head node NFS server.
ssh_config                  # Enable keys between nodes. After nfs_config.
scheduler_config            # Configure scheduler.  After ssh_config.
resource_limits             # Configure login.conf.  After nfs_config.
ganglia_config              # Cluster monitoring.

# Configuration steps that only occur on certain node types
case $NODE_TYPE in
"head")
    startup_scripts
    cluster-update-all-ssh_config
    ;;

"compute")
    # Install after scheduler.  OpenMPI port has options for scheduler
    # integration.
    install_mpi
    rsh_config
    ssh $head_host cluster-update-all-ssh_config
    ;;
*)
    ;;
esac

# Reboot to test new config after initial setup
reboot_maybe

