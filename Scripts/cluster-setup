#!/bin/sh -e

##########################################################################
#   Description:
#       Automate setup of a simple FreeBSD cluster.
#       Installs common software and configures settings to facilitate
#       cluster operation.  Assumes a single head node will handle
#       job initiation/scheduling and act as a file server for all nodes.
#
#   Usage:
#       First run 
#
#           cluster-setup head
#
#       on the head node.  Then run
#
#           cluster-setup compute
#
#       on the rest.
#       
#   History:
#       Dec 2009    J Bacon
##########################################################################


usage()
{
    printf "Usage: $0 head|compute\n"
    exit 1
}


line()
{
    printf "==============================================================================\n"
}


pause()
{
    printf "Press return to continue..."
    read junk
}


stop_pbs()
{
    printf "Stopping PBS daemons...\n"
    if fgrep -q pbs_sched_enable /etc/rc.conf; then
	if /usr/local/etc/rc.d/pbs_sched stop 2> /dev/null; then
	    printf "Stopped pbs_sched.\n"
	fi
	sleep 2
    fi
    if fgrep -q pbs_server_enable /etc/rc.conf; then
	if /usr/local/etc/rc.d/pbs_server stop 2> /dev/null; then
	    printf "Stopped pbs_server.\n"
	fi
	sleep 2
    fi
    if killall pbs_sched 2> /dev/null; then
	printf "Rogue pbs_sched terminated.\n"
	sleep 2
    fi
    if killall pbs_server 2> /dev/null; then
	printf "Rogue pbs_server terminated.\n"
	sleep 2
    fi
}


##########################################################################
#   Description:
#       Run command given by arguments and log output.
##########################################################################

log_cmd()
{
    if [ $# -lt 1 ]; then
	printf "Usage: $0 command [args]\n"
	exit 1
    fi
    log_dir=$START_DIR/desktop-installer-log
    if [ ! -e $log_dir ]; then
	mkdir $log_dir
    fi
    $* 2>&1 | tee -a $log_dir/$1.log
}


##########################################################################
#   Description:
#       Install ports or packages specified by arguments if they are not
#       already installed.  If $USE_PORTS is true, installs from source,
#       otherwise attempts to install using pkg_add.
#
#   Arguments:
#       List of ports in the for "category/name"
##########################################################################

install_packages()
{
    for pkg in $*; do
	log_cmd auto-install-packages $pkg
    done
}


install_port()
{
    if [ $# != 2 ]; then
	printf "Usage: install_port category port\n"
	exit 1
    fi
    
    category=$1
    port=$2
    # FIXME: Use cluster-pkgdir
    if [ -e $head_usr/ports/packages/All ] && \
	ls $head_usr/ports/packages/All/${port}* ; then
	printf "\nInstalling $port from package...\n\n"
	pause
	pkg_add $head_usr/ports/packages/All/${port}*
    else
	printf "\nInstalling $port from port...\n\n"
	pause
	cd /usr/ports/$category/$port
	make deinstall
	printf "Clean port and rebuild? (y/n) [n] "
	read resp
	if [ 0$resp = 0'y' ]; then
	    make clean
	    make rmconfig
	fi
	make reinstall
	pause
    fi
}


add_node_name()
{
    if [ $# != 2 ]; then
	printf "Usage: $0 node-name node-file\n"
	exit 1
    fi
    
    name=$1
    file=$2
    
    # Prevent fgrep from failing
    if [ ! -e $file ]; then
	touch $file
    fi
    
    if ! fgrep -q $name $file; then
	printf "$name\n" >> $file
    fi
}


edit_hosts()
{
    line
    printf "Generating /etc/hosts file...\n"
    if [ $node_type = "compute" ]; then
	# If cluster-setup was previously completed on this node,
	# get head node from configuration, otherwise ask.
	if [ -e $head_usr/local/cluster/head_node ]; then
	    head_host=`cat $head_usr/local/cluster/head_node`
	else
	    head_host=''
	    while [ 0$head_host = 0 ]; do
		head_host=`auto-ask head-hostname "Short hostname of head node?" unknown`
	    done
	fi
	head_host_entry="192.168.0.2\t\t$head_host.local $head_host\n"
    else
	head_host=`hostname -s`
	hostname -s > $PREFIX/cluster/head_node
	head_host_entry="192.168.0.2\t\t$head_host.local\n"
    fi
    
    if ! fgrep -q 'Entries generated by cluster-setup' /etc/hosts; then
	printf "\n# Entries generated by cluster-setup\n" >> /etc/hosts
	printf "$head_host_entry" >> /etc/hosts
	node=1
	while [ $node -lt 245 ]; do
	    ip=$((node+10))
	    printf "192.168.0.%d\t\tcompute-%03d.local compute-%03d\n" \
		$ip $node $node >> /etc/hosts
	    node=$((node+1))
	done
    fi
}


install_scheduler()
{
    line
    # Patch bug in openmpi run_depends
    sed -i '.bak' 's|${SGE_ROOT}/fbsd-${ARCH}/qrsh|${SGE_ROOT}/bin/fbsd-${ARCH}/qrsh|g' \
	/usr/ports/net/openmpi/Makefile
    
    cat << EOM
Please choose a scheduler:

1.. SGE (Requires X11)
2.. Torque PBS

EOM

    resp=`auto-ask scheduler "Enter 1 or 2" unknown`
    case $resp in
	1)
	    if ! auto-package-installed sge; then
		install_packages sysutils/sge62
		
		# Set up host as head node in SGE.
		printf "Ignore the post-install messages above for now.\n"
		printf "We will run install_qmaster later...\n"
	    
		# You may verify your administrative hosts with the command
		# qconf -sh
		# and you may add new administrative hosts with the command
		# qconf -ah <hostname>
		printf "Host configuration can be changed later with qconf.\n\n"
    
		# Source $PREFIX/sge/default/common/settings.csh
		# All execution hosts must be administrative hosts during the installation.
		# All hosts which you added to the list of administrative hosts during this
		# installation procedure can now be installed.
		printf "\nAll users should source $PREFIX/sge/<cell>/common/settings.[c]sh\n\n"
		pause
	    fi
	    ;;
	2)
	    if ! auto-package-installed torque; then
		printf "Installing torque from package...\n"
		install_packages sysutils/torque devel/gmake
		# Package does not set permissions on spool directories
		# Also, users are encouraged to update sysutils/torque
		# to the latest version instead of using the package
		pkg_delete /var/db/pkg/torque*
		auto-install-packages lang/perl devel/libtool
		printf "Reinstalling torque from port...\n"
		install_port sysutils torque
	    fi
	    ;;
	*)
	    printf "\nInvalid selection.\n\n"
	    pause
	    ;;
    esac
}


install_mpi()
{
    line
    # Install OpenMPI from source so we can choose build options
    if ! auto-package-installed openmpi; then
	if [ $USE_PORTS = 0 ]; then
	    printf "\nInstalling openmpi dependencies from packages...\n"
	    install_packages net/openmpi
	    pkg_delete /var/db/pkg/openmpi*
	fi
	printf "Reinstalling openmpi from port to customize build...\n"
	printf "Select your scheduler in the options menu.\n"
	install_port net openmpi
    fi
}


scheduler_config()
{
    line
    # SGE is reinstalled as a dependency of openmpi
    if auto-package-installed sge; then
	printf "\nUpdating /etc/services for SGE...\n"
	# Check /etc/services
	if ! fgrep -q sge_qmaster /etc/services; then
	    printf "sge_qmaster     6444/tcp   #Grid Engine Qmaster Service\n" >> /etc/services
	    printf "sge_qmaster     6444/udp   #Grid Engine Qmaster Service\n" >> /etc/services
	fi
	if ! fgrep -q sge_qmaster /etc/services; then
	    printf "sge_execd       6445/tcp   #Grid Engine Execution Service\n" >> /etc/services
	    printf "sge_execd       6445/udp   #Grid Engine Execution Service\n" >> /etc/services
	fi
	
	# Look into this later...
	# Install PVM utilities if desired
	#printf "Install SGE PVM utils? (y/n) "
	#read resp
	#if [ 0$resp = 0'y' ]; then
	#    cd $PREFIX/sge/pvm/src
	#    ./aimk
	#    ./install.sh
	#    cd $start_dir
	#fi
    fi
    
    if auto-package-installed torque; then
	tspool="/var/spool/torque"
	if [ ! -e $tspool ]; then
	    cp -Rp $PREFIX/share/examples/torque$tspool $tspool
	fi
	
	# Package missing arrays dir
	mkdir -p $tspool/server_priv/arrays
	# Oversight in older ports.  Fixed in 2.4.6.
	# chmod 1777 /var/spool/torque/spool /var/spool/torque/undelivered
    
	# On compute nodes, put '$pbsserver ip.address.of.server' into
	# /var/spool/torque/mom_priv/config
	
	#head_ip=`fgrep $head_host /etc/hosts | awk ' { print $1 }'`
	#printf "$head_ip\n"
	
	##################################################################
	# Head node (torque server) only
	#
	
	if [ $node_type = "head" ]; then
	    # Use main interface address on server if there are multiple
	    # interfaces, not the local interface!  Torque will get
	    # confused otherwise.
	    # Also use short hostname, which must be in /etc/hosts.
	    # If someone messes up /etc/hosts, we prefer a clear lookup
	    # failure over getting the wrong address from DNS.
	    hostname -s > $tspool/server_name
    
	    # Cited as useful for dual nic servers, but not needed
	    #printf "SERVERHOST\t$head_host\n" > $tspool/torque.cfg
	    #$EDITOR $tspool/torque.cfg
    
	    # To configure queues on server, execute
	    # $PREFIX/share/examples/torque/torque.setup <admin>
	    stop_pbs

	    # Annihilates server_priv/nodes.  Edit nodes AFTER this!
	    printf "Running torque.setup...\n"
	    printf "y\n" | $PREFIX/share/examples/torque/torque.setup root > /dev/null
	    sleep 5
	    stop_pbs

	    line
	    nodes_file=$tspool/server_priv/nodes
	    nodes_backup=$PREFIX/cluster/server_priv-nodes
	    if [ -e $nodes_backup ]; then
		cp $nodes_backup $nodes_file
	    fi
	    cat << EOM

From another shell, edit $nodes_file

Add names of existing compute nodes and options.  Compute nodes that are
not yet operational should not be added at this time.

EOM
	    pause
	    cp $nodes_file $nodes_backup

	    # auto-enable-service doesn't start services that are already
	    # configured in rc.conf
	    auto-enable-service pbs_server cluster-setup
	    sleep 2
	    $PREFIX/etc/rc.d/pbs_server restart 2> /dev/null
	    sleep 2

	    auto-enable-service pbs_sched cluster-setup
	    sleep 2
	    $PREFIX/etc/rc.d/pbs_sched restart 2> /dev/null
	    sleep 2
	else
	    # Use local address of head node on compute nodes.  If
	    # head node has multiple interfaces, the DNS entry will
	    # provide the public interface IP rather than the private
	    # cluster IP.
	    # Also use short hostname, which must be in /etc/hosts.
	    # If someone messes up /etc/hosts, we prefer a clear lookup
	    # failure over getting the wrong address from DNS.
	    printf "$head_host\n" > $tspool/server_name
    
	    # On compute nodes, add the following line to /etc/rc.conf:
	    # pbs_mom_enable="YES"
	    line
	    printf "\nAdd `hostname -s` to server_priv/nodes on the server\n"
	    printf "and restart pbs_server.\n\n"
	    pause
	    if fgrep -q pbs_mom /etc/rc.conf; then
		if ! $PREFIX/etc/rc.d/pbs_mom stop; then
		    if killall pbs_mom 2> /dev/null; then
			printf "Old pbs_mom terminated.\n"
		    fi
		fi
	    fi
	    auto-enable-service pbs_mom cluster-setup
	    $PREFIX/etc/rc.d/pbs_mom restart
	    printf "\nTest this node by running 'pbsnodes' on head node.\n\n"
	    pause
	fi
	
	# Change torque/spool and torque/undelivered to 777?
	# Does not appear necessary anymore.
    fi
}


# Currently must come after Torque setup.  Reorg to consolidate scheduler conf
nfs_config()
{
    # Share space on head node
    line
    share="/usr"
    
    case $node_type in
    "head")
	resp=`auto-ask share-usr "Share $share via NFS? (y/n)" y`
	if [ 0$resp = 0y ]; then
	    # Export /usr (which should include /usr/home)
	    exported=`awk '$1 == "'$share'" { print $1 }' /etc/exports`
	    if [ 0$exported = 0 ]; then
		# FIXME: Make sure NFS server is enabled in rc.conf first
		ip=`auto-ask local-ip "IP address of local interface?" unknown`
		# Does not work for multi-homed host
		# ip=`fgrep $head_host /etc/hosts | awk ' { print $1 }' | uniq`
		net=${ip%.[0-9]*}
		printf "Updating /etc/exports...\n"
		printf "# Generated by $0.\n" >> /etc/exports
		printf "$share\t-maproot=0\t-network $net.0 -mask 255.255.255.0\n" >> /etc/exports
		/etc/rc.d/nfsd restart
		killall -HUP mountd
		printf "\nYou may need to reboot in order to mount the new shared folder.\n\n"
		pause
	    else
		printf "$share is already configured in /etc/exports.\n"
	    fi
	fi
	;;
	
    "compute")
    
	# Configure shared directory
	found=`awk '$1 == "'$head_host:$share'" { print $1 }' /etc/fstab`
	if [ 0$found = 0 ]; then
	    printf "\nUpdating /etc/fstab...\n\n"
	    printf "# Generated by $0.\n" >> /etc/fstab
	    printf "$head_host:$share\t\t$head_usr\tnfs\trw,intr,soft\t0\t0\n" >> /etc/fstab
	    mkdir -p $head_usr
	    mount $head_usr
	    pause
	else
	    printf "Remote filesystem $head_host:$share already configured.\n"
	fi
    
	# Add name of node to master list
	add_node_name `hostname -s` $head_usr/local/cluster/compute_nodes
	if auto-package-installed torque; then
	    add_node_name `hostname -s` $head_usr/local/cluster/torque_nodes
	fi
	
	# Share home directories
	resp=`auto-ask shared-home "\nConfigure shared home directories? (y/n)" y`
	if [ 0$resp = 0'y' ]; then
	    if [ -e $head_usr/home ]; then
		rm -f /home
		ln -s $head_usr/home /
	    fi
	else
	    printf "\nBe sure to create a shared data directory for users.\n\n"
	    pause
	fi
	;;
	
    *)
	;;
    esac
}


# Must come after NFS setup
sge_config()
{
    case $node_type in
    "head")
	# Configure SGE master
	if auto-package-installed sge; then
	    #cd $PREFIX/sge
	    # -csp is for added security
	    #./install_qmaster -csp
	    #cd $start_dir
	    cell=`auto-ask sge-cell "SGE cell name [default='default']?" default`
	    if [ 0$cell = 0 ]; then
		cell='default'
	    fi
	    
	    if ! fgrep -q "sge_cell" /etc/rc.conf; then
		printf "sge_cell=\"$cell\"\n" >> /etc/rc.conf
	    fi
	    if ! fgrep -q "sge_qmaster" /etc/rc.conf; then
		printf "sge_qmaster_enable=\"YES\"\n" >> /etc/rc.conf
	    fi
	    
	    if [ ! -e $PREFIX/sge/$cell ]; then
		printf "\nRunning $PREFIX/sge/install_qmaster...\n\n"
		pause
		save_cwd=`pwd`
		cd $PREFIX/sge
		./install_qmaster
		cd $save_cwd
	    fi
	fi
	;;
    compute)
	# Configure SGE node
	if auto-package-installed sge; then
	    #cd $PREFIX/sge
	    #./install_execd
	    #cd $start_dir
	    
	    cell=`auto-ask sge-cell "SGE cell name [default='default']?" default`
	    if [ 0$cell = 0 ]; then
		cell='default'
	    fi
	    if ! fgrep -q "sge_cell" /etc/rc.conf; then
		printf "sge_cell=\"$cell\"\n" >> /etc/rc.conf
	    fi
	    if ! fgrep -q "sge_execd" /etc/rc.conf; then
		printf "sge_execd_enable=\"YES\"\n" >> /etc/rc.conf
	    fi
	    
	    # Link cell directory or whole SGE directory from head to
	    # $PREFIX/sge
	    ln -sf $head_usr/local/sge/$cell $PREFIX/sge
	    
	    # 
	    cat << EOM
	
Ready to run SGE setup tool...
Make sure $(hostname) is configured as an administrative host
On head node: Run qconf -sh to check and qconf -ah to add.

EOM
	    pause
    
	    #if [ ! -e $PREFIX/sge/$cell ]; then
		printf "\nRunning $PREFIX/sge/install_execd...\n\n"
		pause
		save_cwd=`pwd`
		cd $PREFIX/sge
		./install_execd
		cd $save_cwd
	    #fi
	fi
	;;
    esac
}


ganglia_config()
{
    if [ $node_type = "head" ]; then
	# Install apache
	printf "\nConfiguring apache...\n\n"
	if ! auto-package-installed apache; then
	    install_packages www/apache22
	    pause
	fi
	
	auto-enable-service apache22 cluster-setup
	
	# Check for php5 apache module
	if ! auto-package-installed php5; then
	    # Can't use install_packages: dir name does not match port name
	    if [ ! -e $PREFIX/bin/gm4 ]; then
		install_packages devel/m4
	    fi
	    cd /usr/ports/lang/php5
	    printf "\nInstalling PHP5 for Ganglia...\n"
	    make -DBATCH -DWITH_APACHE deinstall clean reinstall
	    pause
	fi

	if ! auto-package-installed ganglia-webfrontend; then
	    install_packages sysutils/ganglia-webfrontend
	    printf "\nCopy and paste the example above into $PREFIX/etc/apache22/httpd.conf.\n\n"
	    pause
	fi
	cat << EOM

From another shell, add index.php to dir_module, e.g.
    
    <IfModule dir_module>
	DirectoryIndex index.php
	DirectoryIndex index.html
    </IfModule>
    
    and the following to mime_module:
    
    <IfModule mime_module>
	AddType application/x-httpd-php .php
	AddType application/x-httpd-php-source .phps
    </IfModule>
    
EOM
	pause
	
	# Add date_default_timezone_set('America/Chicago'); to ganglia.php
	# or update date.timezone in $PREFIX/etc/php.ini and restart
	# Apache
	if [ ! -e $PREFIX/etc/php.ini ]; then
	    cp $PREFIX/etc/php.ini-production $PREFIX/etc/php.ini
	fi
	cat << EOM

From another shell, set date.timezone in $PREFIX/etc/php.ini

Example:

date.timezone = "America/Chicago"

EOM
	pause
	$PREFIX/etc/rc.d/apache* restart
	
	# Configure ganglia
	# FIXME: Determine exact configuration necessary for running
	# on a multi-homed head node.
	cat << EOM

From another shell, edit $PREFIX/etc/gmetad.conf:

At least set data_source and gridname using the examples provided in
the comments.  Example:

    data_source "head node" localhost
    data_source "compute nodes" compute-001 compute-002 compute-003
    gridname "peregrine"

For multi-homed hosts, you must also specify the local IP address in
the data_source line and in trusted_hosts, e.g.

    trusted_hosts 127.0.0.1 192.168.0.2 peregrine.hpc.uwm.edu
    
EOM
	pause
    fi
    
    line
    if ! auto-package-installed ganglia-monitor-core; then
	install_packages sysutils/ganglia-monitor-core
	pause
    fi
    
    cat << EOM
    
From another shell, edit $PREFIX/etc/gmond.conf:

At least set the name field in the cluster section to match the name
in gmetad.conf on the head node.

    cluster { 
      name = "peregrine" 
      owner = "unspecified" 
      latlong = "unspecified" 
      url = "unspecified" 
    } 
EOM
    pause
    
    auto-enable-service gmond cluster-setup
    
    if [ $node_type = "head" ]; then
	auto-enable-service gmetad cluster-setup
    fi
    
    printf "\nTest this node by browsing to http://$head_host/ganglia/\n\n"
    pause
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

ssh_config()
{
    # Verify that nfs_config has been run
    
    line
    
    # 
    if [ $node_type = 'head' ]; then
	# Generate ssh keys
	printf "\nGenerating ssh keys for the head node...\n\n"
	id_dsa="$cluster_data/id_dsa"
	if [ ! -e $id_dsa ]; then
	    rm -rf ~/.ssh
	    ssh-keygen -t dsa -f ~/.ssh/id_dsa -N ''
	    rm -f $cluster_data/id_dsa*
	    cp -p ~/.ssh/id_dsa* $cluster_data
	    chmod 600 $cluster_data/id_dsa*
	fi
	
	# Disable prompting for new hosts, etc.
	cluster-update-ssh_config
    else
	printf "Configuring ssh...\n"
	
	# Install ssh keys from head node
	if [ -e $head_usr/local/cluster/id_dsa.pub ]; then
	    mkdir -p ~/.ssh
	    chmod 0700 ~/.ssh
	    cp $head_usr/local/cluster/id_dsa.pub ~/.ssh/authorized_keys2
	fi
	
	# Permit root login over ssh
	sed -i ".bak" 's|#PermitRootLogin no|PermitRootLogin yes|g' /etc/ssh/sshd_config
	cat << EOM
    
You must run 'killall -HUP sshd' before ssh will allow root logins.
Note: Doing it now might terminate this shell.

EOM
    fi
    pause
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

resource_limits()
{
    # Set resource limits
    line
    if [ ! -e /etc/login.conf.orig ]; then
	cp /etc/login.conf /etc/login.conf.orig
    fi
    if [ -e $head_usr/local/cluster/login.conf ]; then
	# FIXME: replace this if with a function
	cp $head_usr/local/cluster/login.conf /etc
    else
	cat << EOM

From another shell, edit /etc/login.conf:

Set desired resource limits for the head node in /etc/login.conf.

It is a good idea to limit vmemoryuse and maxproc on the head node
to prevent it from being overloaded by careless users.

The scheduler should manage limits on compute nodes, but some generous hard
limits on vmemoryuse and maxproc in login.conf can help protect nodes from
fork() bombs and memory leaks that the scheduler does not catch (fast enough).

EOM
	pause
	cap_mkdb /etc/login.conf
	if [ $node_type = 'compute' ]; then
	    cp /etc/login.conf $head_usr/local/cluster
	fi
    fi
}


##########################################################################
#   Function description:
#       
#   Arguments:
#       
#   Returns:
#       
#   History:
#   Date        Name        Modification
#   2012-07-30  Charlie &   Begin
##########################################################################

rsh_config()
{
    # Enable rsh
    resp=`auto-ask enable-rshd "Enable rshd (some MPI programs use it by default)? (y/n)" y`
    if [ 0$resp = 0'y' ]; then
	cat << EOM

Editing /etc/inetd.conf:

Uncomment both rshd and rlogind.

EOM
	pause
	$EDITOR /etc/inetd.conf
	# Add inetd to rc.conf
	if ! fgrep inetd_enable /etc/rc.conf; then
	    printf 'inetd_enable="YES"\n' >> /etc/rc.conf
	fi
	/etc/rc.d/inetd restart
	# FIXME: Use auto-append-line to add to /etc/hosts.equiv
	printf "$head_host\n" >> ~/.rhosts
    fi
}

##########################################################################
#   Main
##########################################################################

if [ $# != 1 ]; then
    usage $0
fi

if [ `whoami` != root ]; then
    printf "$0 must be run as root.\n"
    exit 1
fi

if /etc/rc.d/ntpd stop; then
    printf "Stopped NTP.\n"
fi
ntpdate pool.ntp.org
auto-enable-service ntpd cluster-setup
/etc/rc.d/ntpd restart
auto-enable-procfs

resp=`auto-ask use-source "Build/install ports from source? (y/n)" n`
if [ 0$resp = 0'y' ]; then
    USE_PORTS=1
else
    USE_PORTS=0
    if [ x$AUTO_PACKAGEROOT = x ]; then
	printf "Finding fastest mirror... "
	export AUTO_PACKAGEROOT=`auto-fastest-mirror`
	printf "$AUTO_PACKAGEROOT\n"
    fi
fi

if [ 0$EDITOR = 0 ]; then
    EDITOR=vi
    export EDITOR
fi

# Quick-install some common tools
install_packages devel/gmake net/rsync converters/libiconv security/sudo

head_usr="/head_usr"
start_dir=`pwd`
PREFIX="/usr/local"
cluster_data="$PREFIX/cluster"
mkdir -p $cluster_data
node_type=$1
case $node_type in
"head"|"compute")
    ;;
*)
    printf "Invalid installation type: $node_type\n"
    usage $0
esac

edit_hosts
install_scheduler
# MPI is not used on the head node, but needed for scheduler config?
# Maybe this should only be installed on compute nodes.
if [ $node_type = 'compute' ]; then
    install_mpi
fi
scheduler_config
nfs_config
ssh_config      # After nfs_config
resource_limits # After nfs_config

if auto-package-installed sge; then
    sge_config
fi
ganglia_config

line
case $node_type in
"head")
    
    # Install extra login logic
    line
    printf "Installing startup scripts...\n"
    if [ ! -e $PREFIX/cluster/profile ]; then
	cp $PREFIX/share/cluster-admin/profile $PREFIX/cluster
    fi
    if [ ! -e $PREFIX/cluster/csh.login ]; then
	cp $PREFIX/share/cluster-admin/csh.login $PREFIX/cluster
    fi
    auto-append-line cluster/profile ". $PREFIX/cluster/profile" /etc/profile cluster-setup
    auto-append-line cluster/csh.login "source $PREFIX/cluster/csh.login" /etc/csh.login cluster-setup
    ;;

"compute")

    rsh_config
    ;;
*)
    ;;
esac

cat << EOM

=======================================================================
			**** IMPORTANT ****
			
Be sure to run cluster-update--ssh_config on the head node and all
compute nodes AFTER all compute nodes are configured.
=======================================================================

EOM

